\clearpage
\chapter{Methodology}

\section{Dataset }
This study utilised video recordings of skateboarders performing tricks as its
primary data source. To ensure model robustness, the final dataset consisted of
videos across diverse environmental conditions and varying skateboarder skill
levels.

The initial dataset was sourced from the publicly available "SkateboardML"
repository on GitHub \cite{lightningdrop2020skateboardml}, comprising 200 video
clips corresponding to two common tricks: the ollie and the kickflip. This
dataset laid a solid foundation for this research. However, to expand the
dataset's diversity, additional data was obtained through direct communication
with Hanxiao Chen, the author of the SkateboardAI paper
\cite{SkateboardAIPaper}. This communication yielded a dataset containing 750
videos covering 15 distinct tricks. Given the wide range of tricks included in
this dataset, many were beyond the scope of this study, therefore only a subset
of these videos were selected and included into the final dataset.

\section{Class Establishment}
In the development of a skateboard trick classifier, this study pursued a
multi-class classification strategy, targeting three fundamental skateboard
tricks: ollie, pop shuvit and kickflip. These tricks were selected on the basis
of two primary criteria. Firstly, they are often associated with the first
tricks learnt by beginners, highlighting their role in foundational
skateboarding skills. Secondly, their popularity within the skateboarding
community, often performed in competitions emphasises their relevance, making
them highly relevant for analysing and evaluating competitive performance.


\section{Data Preparation}

\subsection{Labelling techniques}
This study investigated two primary labelling techniques: the folder-based
approach and the text-based approach. In the folder-based method, videos were
categorised into folders named after their corresponding class label offering a
simple organisation method. On the other hand, in the text-based approach, each
video's path and corresponding label were listed on a text file, providing more
flexibility. Given the limited number of classes and manageable dataset size,
this study chose to utilise the folder-based approach, as the extra complexity
from the text-based method wasn't necessary for this project.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.55\textwidth}
	\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/folder-based-label.jpg}
	\caption{Folder-based labelling.}
	\label{fig:folder-based-label}
\end{minipage}
\hfill
\begin{minipage}[t]{0.35\textwidth}
	\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/text-based-labelling.jpg}
	\caption{Text-based labelling}
	\label{fig:text-based-label}
\end{minipage}
\end{figure}

\subsection{Frame Extraction}

Given that the nature of this research is an activity recognition task, it employed frame extraction to convert video into a suitable format that the ML models can interpret. This technique involved extracting relevant frames from a video in such a way that it encapsulates the entire skateboard trick including the wind up, the manoeuvrer and the landing. Two primary techniques were explored, the first being uniform sampling, where frames are extracted at regular intervals based on a pre-calculated step size, ensuring a consistent division of frames across the video, regardless of its length.
The second technique involved using optical flow to % continue on optical flow

% given that this is a video recognition task - we require the use of extracting frames

% Talk about the differnet techniques of frame extractin due to the short vide
% uniform sampling
% think about using optical flow to extract frames that show motion
% clips -- figuring out how many extra frames required, then duplicating at the
% end, -- also tried duplicating randomly in the middle, which did not produce
% good results

%Yes, you should definitely mention the two frame extraction methods you explored in the methodology section. Here's how to approach it and what you would include in both methodology and implementation:
%
%Methodology
%
%Introduce the need for frame extraction: Briefly reiterate the importance of frame extraction for video recognition tasks, especially when dealing with activity recognition.
%Describe the two techniques:
%Optical Flow: Explain the concept of using optical flow to calculate motion, and how you utilized it to assign weights to frames, prioritizing those with higher motion values for selection.
%Uniform Sampling: Describe the process of extracting frames at regular intervals and how you applied it.
%Evaluation Criteria: Mention the criteria used to compare the techniques (e.g., ability to capture key actions, reduce redundancy, maintain temporal consistency).
%Justification of Choice: Briefly state the method that performed better based on your evaluation criteria and explain the reason for its selection.
%
%Implementation
%
%Chosen Method: State the chosen technique (e.g., optical flow-based extraction).
%Software/Libraries: Mention the specific tools used to implement this (e.g., OpenCV).
%Specific Parameters: Describe any key parameters or thresholds you configured for the optical flow calculation and frame selection.
%Integration: Briefly explain how the extracted frames were used in your activity recognition model (e.g., input into a convolutional neural network).
%
%Discussion of Results
%
%Whether you should include a detailed discussion of results for both techniques depends on the scope of your thesis and how central frame extraction is to your core research findings. Here are some options:
%
%Core Focus: If frame extraction is a crucial element, include a comparative analysis of the results obtained from both methods and how they influenced your final choice.
%Secondary Focus: If frame extraction is less central, briefly summarize the outcome in the methodology section, stating why the chosen method yielded better results for your task.
%Results Section: You can dedicate a subsection in your separate results section to discuss the performance of both frame extraction techniques in more detail.


\subsection{Data Augmentation}
Due to the limited number of data used in this research, data augmentation
techniques were implemented to increase the dataset's size prior to model
training. These techniques including rotating, flipping and adding noise to
images also help reduce overfitting, which was a risk evident due to the
over-parametrised nature of the chosen models, meaning that their number of
trainable parameters surpass the size of the dataset, making it very vulnerable
to overfitting.


\section{Preprocessing}
\subsection{}



\section{ML Architectures}

\subsection{Feature Extraction models}

\subsection{Main models}



% VGG-LSTM - because of paper from lit review
% Resnet50-LSTM
% Resnet50-BiLSTM - becuase of skateboard paper and lit review paper

\section{Evaluation Methods}


