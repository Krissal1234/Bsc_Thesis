\clearpage

\chapter{Methodology}\label{4_methodology}
This chapter explores the design and methodologies used for the proposed
artefact, including the overall architecture, data preparation and
preprocessing, feature extraction and the models adapted for this study. Various
techniques are investigated, and the final approaches are elaborated
upon.

\section{Class Establishment}
This study pursued a multi-class classification strategy, targeting three
fundamental skateboard tricks: ollie, pop-shuvit and kickflip. These tricks were
selected based on two primary criteria. Firstly, they are often associated with
the first tricks learnt by beginners, highlighting their role in foundational
skateboarding skills. Secondly, their popularity within the skateboarding
community, often performed in competitions emphasises their relevance, making
them highly relevant for analysing and evaluating competitive performance.


\section{Data Preparation}

\subsection{Dataset}
This study utilised video recordings of skateboarders performing tricks as its
primary data source. To ensure model robustness, the final dataset consisted of
videos across diverse environmental conditions and varying skateboarder skill
levels.

The initial dataset was sourced from the publicly available `SkateboardML'
repository on GitHub~\cite{lightningdrop2020skateboardml}, comprising 200 video
clips corresponding to two common tricks: the ollie and the kickflip. To expand
the dataset's diversity, additional data was obtained through direct
communication with Hanxiao Chen, the author of the SkateboardAI
paper~\cite{SkateboardAIPaper}. This communication yielded a dataset containing
750 videos covering 15 tricks. However, since the majority of tricks were beyond
the scope of this study, only a subset of this data was included into the final
dataset. Ultimately, the final dataset comprised of 128 videos per class,
totalling 384 videos.

\subsection{Labelling techniques}
This study explores two primary labelling techniques: the folder-based and the
text-based approach as illustrated in Figures~\ref{fig:folder-based-label}
and~\ref{fig:text-based-label}. The folder-based method categorises videos into
folders named after their corresponding class label offering a simple
organisation method. On the other hand, the text-based approach, lists each
video's path and corresponding label in a text file, providing more flexibility.
Given the limited number of classes and manageable dataset size, this study
selected the folder-based approach, considering the extra complexity of the
text-based method unnecessary for this project.
%maybe mention one hot here or in implmenetaiton

\begin{figure}[h]
	\centering
	\begin{minipage}[t]{0.55\textwidth}
		\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/folder-based-label.jpg}
		\caption{Folder-based labelling.}\label{fig:folder-based-label}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/text-based-labelling.jpg}
		\caption{Text-based labelling}\label{fig:text-based-label}
	\end{minipage}
\end{figure}

\vspace{-1cm} % hspace for horizontal
\section{Preprocessing}

\subsection{Frame Extraction}
% Debating this section with the other section below it
Recognising complex human actions requires the examination of sequential data as
opposed to relying on single frames or
images~\cite{ActionRecognitionDeepBi-DirectionalLSTM}. To illustrate this point,
consider the example of a skateboarder executing a skateboard trick like the
`Kickflip', as depicted in Figure~\ref{fig:SingleVsMultiFrameKickflip}. By
feeding an AI model a single image of this trick as shown in
Figure~\ref{fig:singleFrameKickflip}, it may misinterpret the manoeuvre as
another trick due to its subjective nature. Whereas, by providing the model with
a sequence of frames, as illustrated in Figure~\ref{fig:MultiFrameKickflip}, the
entire trick sequence is captured including the wind-up, the trick and the
landing, capturing actions such as foot placement and board rotation, which
together characterise the full skateboard trick.

\begin{figure}[h]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.57\linewidth]{content/chapters/2_background/figures/Tricks/single.png}
    \caption{A Single Frame of a `Kickflip'.}\label{fig:singleFrameKickflip}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{content/chapters/2_background/figures/Tricks/KickflipFrames.PNG}
    \caption{Multiple Frames of a `Kickflip'.}\label{fig:MultiFrameKickflip}
  \end{subfigure}
  \caption{Comparison of a Single Frame and Multiple Sequential Frames.}\label{fig:SingleVsMultiFrameKickflip}
\end{figure}

% Given that the nature of this research is an activity recognition task, this
% study employed frame extraction to convert videos into sequences of frames
% suitable for processing by ML models. This technique aims to extract a subset of
% frames that encapsulate the essential dynamics of the activity. Thus, in the
% context of skateboard trick recognition, the frames should capture the entire
% sequence, including the wind-up, the trick and the landing.

%TODO: change description of optical flow to match with its application in this study i.e. skatbeoarding
This study explored two primary frame extraction techniques: uniform sampling
and optical flow method. The former technique evenly splits a video into its
corresponding frames by using a pre-calculated step size determined by the
number of frames of a video. The second technique leverages optical flow to
select the frames with the most significant movement. This method calculates the
motion of pixels between successive frames and assigns weights to each frame
determined by the magnitude of this motion.
Figure~\ref{fig:kickflip-sequence-opticalflow} illustrates a visualisation of
this technique being applied to `kickflip' trick.

% a method that assigns weights to frames determined by the motion of
% pixels between them. With this approach, frames with the greatest weightings are
% chosen for extraction, resulting in capturing frames with the most movement.
% Figure~\ref{fig:kickflip-sequence-opticalflow} illustrates a visualisation of
% this technique being applied to a skate trick.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/kickflip_sequence_opticalFlow.jpg}
        \caption{Optical flow visualisation of a kickflip sequence, highlighting regions with significant motion.}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/kickflip_sequence.jpg}
        \caption{Extracted frames from the sequence based on the greatest weightings from the optical flow representation.}
    \end{subfigure}

    \caption{Visualisation of optical flow: (a) Optical flow representation (b)
    Individual frames extracted.}\label{fig:kickflip-sequence-opticalflow}
\end{figure}



\subsection{Data Augmentation}
The limited number of data available for this research necessitated the
implementation of data augmentation techniques to enlarge the dataset's size
before model training. Techniques such as rotating, flipping and adding noise to
images also served to add more diversity to the dataset mitigating the risk of
overfitting. This risk arises from the over-parameterised nature of the
chosen models, whose trainable parameter count surpasses the
size of the dataset, causing them to be vulnerable to overfitting.

\subsection{Normalisation}
During the preprocessing stage, normalisation played a crucial role in preparing
the video frame data before further processing. In particular this study
utilised min-max normalisation to scale the pixel intensities between 0 and 1.
Normalising data in this manner ensured that the model's input values data was
within a standardised range, calculated by Equation~\ref{eq:normalisation}.

\begin{equation}
	\label{eq:normalisation}
	\text{Normalized Value} = \frac{\text{Pixel Value} - \text{Min Value}}{\text{Max Value} - \text{Min Value}}
\end{equation}


\subsection{Feature Extraction with Transfer Learning}
%Feature extraction plays a crucial role in the classification of complex data,
%particularly in the domain of action recognition tasks~\cite{SkateboardAIPaper,
%HARUsingTransferLearningWithDeepRepresentations,skatePaper1}. This study
%incorporates transfer learning by utilising pre-trained CNNs to extract
%relevant features from video frames before passing them through the main
%sequence modelm.

This research leveraged two well-established CNNs for this task: VGG16  and
ResNet50. Both models were pre-trained on the large ImageNet
dataset~\cite{imageNetDataset}, leveraging extensive image recognition knowledge
that were then fine-tuned to the domain of skateboard trick recognition.
Table~\ref{tab:TL-models} summarises the key characteristics of VGG16 and
ResNet50.

\vspace{3mm}
{\bf VGG16:} VGG16, introduced in 2014 by Simonyan and Zisserman~\cite{VGG}, is
a CNN architecture known for its success in image classification and feature
extraction for Action Recognition tasks. This model is relatively simple made up
of 16 convolutional and fully connected (FC) layers with repeated use of 3$\times$3
convolutional filters to preserve spatial information.
Furthermore, VGG16 has proved to be effective for feature extraction in previous
literature~\cite{SkateboardAIPaper}, supporting its suitability in this study.

\vspace{3mm}
{\bf Resnet50:} Resnet50, introduced in 2016 by He et al.~\cite{Resnet50}, is
another powerful CNN architecture that is well known for its advancements in
image classification and frame extraction capabilities. Unlike VGG16, this model
has a deeper architecture employing 50 convolutional layers allowing it to learn
more intricate feature representations of the data. Furthermore, it incorporates
residual connections to counteract the vanishing gradient problem that is often
caused by many layers.

\begin{table}[htbp]
	\centering

	\begin{tabularx}{\textwidth}{>{\RaggedRight}Xcc}
		\toprule
		\textbf{Model} & \textbf{VGG16} & \textbf{ResNet50} \\
		\midrule
		\textbf{Published} & 2014 & 2016 \\
		\textbf{Layers} & 16 (Convolutional and FC layers) & 50 (Convolutional layers) \\
		\textbf{Parameters} & $\sim$138 million & $\sim$25 million \\
		\textbf{Input Image Size} & 224$\times$224  & 224$\times$224  \\
		\bottomrule
	\end{tabularx}
	\captionof{table}{Characteristics of Transfer Learning Models VGG17 and ResNet50}\label{tab:TL-models}
\end{table}


The final four fully-connected (FC) layers of these models, which are typically
used for classification were removed. This was crucial to adapt these models for
feature extraction, as the aim was not to classify each frame of the video, but
to extract a collection of features that could help sequence models gain a
better understanding. These features in the form of vectors not only encapsulate
the visual information present in each frame but also more abstract ideas such
as low-level information (like edges/shapes) to higher-level features like
objects and even actions themselves.

\subsection{Dimensionality Reduction}\label{4_dimensionality_reduction}

The use of pre-trained models used in this study generate high-dimensional
feature vectors. While these features capture valuable information, a large
number of dimensions can lead to challenges. Firstly, it increases the
computational costs of training models. Secondly, it could potentially lead to
the `curse of dimensionality' as explained by Venkat
(2018)~\cite{curseofdimensionality}, where models may struggle to learn
effectively with high dimensional data. To address these issues, dimensionality
reduction is employed after feature extraction to reduce the number of features
while still retaining valuable information for classification.

\section{Adapted Models}
This research investigates the performance of four different Deep Learning
architectures for skateboard trick classification. These models leverage
pre-trained CNNs for feature extraction followed by sequence modelling
techniques to capture the temporal dynamics of skateboard tricks. The
architectures investigated are as follows.

\begin{enumerate}
	\item \textbf{VGG16-LSTM:} This architecture leverages the pre-trained VGG16
	model to extract features, which are then connected to an LSTM for sequence
	modelling.
	\item \textbf{ResNet50-LSTM:} This architecture utilises the deeper ResNet50
	pre-trained CNN for feature extraction, followed by an LSTM for sequence
	modelling. The increased depth of ResNet50 potentially allows it to learn
	more complex feature representations than VGG16.
	\item \textbf{VGG16-BiLSTM:} This architecture employs the VGG16 model for
	feature extraction and a BiLSTM network for sequence modelling. BiLSTMs are
	known to be able to learn dependencies in both directions of a sequence,
	which could be useful for capturing subtle details in skateboard tricks.
	\item \textbf{ResNet50-BiLSTM:} This architecture combines ResNet50 for
	feature extraction with a BiLSTM network. This combination leverages the
	potential advantages of deeper feature learning and the bidirectional
	capabilities of BiLSTMs.
\end{enumerate}

The effectiveness of these models in prior research for video-based Action
Recognition tasks motivated their selection for this study. For instance, Orozco
et al. (2020) achieved 93\% accuracy using a VGG-LSTM
architecture~\cite{HARRecognitionInVideosUsingARobustCNNLSTMApproach}.
Additionally, Chen's work~\cite{SkateboardAIPaper} explored all four of these
models in the context of skateboard trick classification and demonstrated the
potential for competitive results.
% While this study also explored attention
% mechanisms within these models, the main aim is to build upon existing knowledge
% and potentially achieve better performance.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/architecture.png}
	\caption{Artefact Architecture.}\label{fig:architecture}
\end{figure}

\section{Architecture}
The Deep Learning Architecture, as illustrated in Figure~\ref{fig:architecture}
demonstrates the data flow pipeline for the skateboard trick classifier. The
process begins with preprocessing the labelled video data including frame
extraction, augmenting the training set for variability and reducing
dimensionality for efficiency. Pre-trained CNNs such as VGG16 and ResNet50, then
extract features from the preprocessed frames, before feeding them into their
respective sequence models.

The accuracy of these models are then evaluated on a held-out test set, to
determine the models' performance on unseen data. Finally, the evaluation utilises
performance plots such as confusion matrices to visualise classification
performance, model epoch loss and accuracy graphs to track training history.

\section{Evaluation Methods}
To thoroughly evaluate the performance of the proposed models, this study
employed various evaluation metrics, including accuracy, precision, recall and
F1-score. These metrics provided valuable insights into the
model's ability to correctly classify different tricks.

The metrics are defined as follows, described in
the context of the `kickflip' class and tabulated in a confusion matrix.

\begin{itemize}
	\item \textbf{True Positive (TP)}: The number of instances that the model correctly identified a video as containing a kickflip.
	\item \textbf{True Negative (TN)}: The number of instances that the model incorrectly identified a video as containing a kickflip.
	\item \textbf{False Positive (FP)}: The number of instances that the model correctly identified a video as not containing a kickflip.
	\item \textbf{False Negative (FN)}: The number of instances that the model incorrectly identified a video as not containing a kickflip.
\end{itemize}


\noindent \textbf{Accuracy:} Measures the proportion of correctly classified
instances, over the total number of predictions made by the model. This metric
provides a generic indicator of the model's reliability, however this specific
metric can be sensitive in scenarios with class imbalance~\cite{classificationAssessmentMethodstharwat2020}.

\begin{equation}
	\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

\noindent \textbf{Precision:} Measures the proportion of predicted positive
instances that are real positives~\cite{Evaluation:fromprecisionrecallandF-measuretoROCinformednessmarkednessandcorrelation}.
In the context of kickflip detection, it represents the proportion of true
kickflips against all predicted kickflips.
\begin{equation}
	\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\noindent \textbf{Recall:} Measures the proportion of positive instances that
are predicted
positive~\cite{Evaluation:fromprecisionrecallandF-measuretoROCinformednessmarkednessandcorrelation}. In
the context of kickflip detection, it represents the proportion of true
kickflips that are identified correctly.
\begin{equation}
	\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\noindent \textbf{F1-score:} This metric provides a balanced measure of
performance by combining both precision and recall. It is calculated using the
harmonic mean on both values and outputs a value ranging from zero to one with
values closer to one, indicating better performance.
\begin{equation}
	\text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}




