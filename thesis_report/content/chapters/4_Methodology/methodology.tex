\clearpage
\chapter{Methodology}

\section{Dataset }
This study utilised video recordings of skateboarders performing tricks as its
primary data source. To ensure model robustness, the final dataset consisted of
videos across diverse environmental conditions and varying skateboarder skill
levels.

The initial dataset was sourced from the publicly available "SkateboardML"
repository on GitHub \cite{lightningdrop2020skateboardml}, comprising 200 video
clips corresponding to two common tricks: the ollie and the kickflip. This
dataset laid a solid foundation for this research. However, to expand the
dataset's diversity, additional data was obtained through direct communication
with Hanxiao Chen, the author of the SkateboardAI paper
\cite{SkateboardAIPaper}. This communication yielded a dataset containing 750
videos covering 15 distinct tricks. Given the wide range of tricks included in
this dataset, many were beyond the scope of this study, therefore only a subset
of these videos were selected and included into the final dataset.

\section{Class Establishment}
In the development of a skateboard trick classifier, this study pursued a
multi-class classification strategy, targeting three fundamental skateboard
tricks: ollie, pop shuvit and kickflip. These tricks were selected on the basis
of two primary criteria. Firstly, they are often associated with the first
tricks learnt by beginners, highlighting their role in foundational
skateboarding skills. Secondly, their popularity within the skateboarding
community, often performed in competitions emphasises their relevance, making
them highly relevant for analysing and evaluating competitive performance.


\section{Data Preparation}

\subsection{Labelling techniques}
This study investigated two primary labelling techniques: the folder-based
approach and the text-based approach. In the folder-based method, videos were
categorised into folders named after their corresponding class label offering a
simple organisation method. On the other hand, in the text-based approach, each
video's path and corresponding label were listed on a text file, providing more
flexibility. Given the limited number of classes and manageable dataset size,
this study chose to utilise the folder-based approach, as the extra complexity
from the text-based method wasn't necessary for this project.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.55\textwidth}
	\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/folder-based-label.jpg}
	\caption{Folder-based labelling.}
	\label{fig:folder-based-label}
\end{minipage}
\hfill
\begin{minipage}[t]{0.35\textwidth}
	\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/text-based-labelling.jpg}
	\caption{Text-based labelling}
	\label{fig:text-based-label}
\end{minipage}
\end{figure}

\section{Preprocessing}

\subsection{Frame Extraction}

Given that the nature of this research is an activity recognition task, this
study employed frame extraction to convert videos into sequences of frames
suitable for processing by ML models. This technique aims to extract a subset of
frames that encapsulate the essential dynamics of the activity. Thus, in the
context of skateboard trick recognition, the frames should capture the entire
sequence, including the wind up, the trick and the landing. 

Two primary frame extraction techniques were explored, uniform sampling and
optical flow method. The former technique, evenly splits a video into its
corresponding frames by using a pre-calculated step size determined by the
number of frames of a video. The second technique leveraged optical flow, a
method that assigns weights to frames determined by the motion of pixels between
them. With this approach, frames with the greatest weightings were chosen for
extraction, resulting in capturing only action-specific frames. 


\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/kickflip_sequence_opticalFlow.jpg}
		\caption{Optical flow visualisation of a kickflip sequence, highlighting regions with significant motion.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/4_Methodology/figures/kickflip_sequence.jpg}
		\caption{Extracted frames from the sequence based on the greatest weightings from the optical flow representation.}
	\end{subfigure}

	\caption{Visualisation of optical flow: (a) Optical flow representation (b) Individual frames extracted.}
	\label{fig:kickflip-sequence-opticalflow}
\end{figure}


\subsection{Data Augmentation}
Due to the limited number of data used in this research, data augmentation
techniques were implemented to increase the dataset's size prior to model
training. These techniques including rotating, flipping and adding noise to
images also help reduce overfitting, which was a risk evident due to the
over-parametrised nature of the chosen models, meaning that their number of
trainable parameters surpass the size of the dataset, making it very vulnerable
to overfitting.


\subsection{Feature Extraction with Transfer Learning}
In order to 

\subsection{Sequence Modelling}
% VGG
% ResNet50
\subsection{Normalisation}
During the preprocessing stage, normalisation played a crucial role in preparing
the video frame data before further processing. In particular this study
utilised min-max normalisation to scale the pixel intensities between 0 and 1.
Normalising data in this manner ensured that the model's input values data was
within a standardised range.

\section{Overall Architectures}


\subsection{Feature Extraction models}



\subsection{Sequence models}



% VGG-LSTM - because of paper from lit review
% Resnet50-LSTM
% Resnet50-BiLSTM - becuase of skateboard paper and lit review paper

\section{Evaluation Methods}
To thoroughly evaluate the performance of the proposed models, this study
employed various evaluation metrics, including accuracy, precision, recall,
F1-score and confusion matrix. These metrics provided valuable insights on the
model's ability to correctly classify different tricks and depend on
the following definitions, described in the context of the kickflip class as an
example.

\begin{itemize}
	\item \textbf{True Positive (TP)}: The model correctly identified a video as containing a kickflip.
	\item \textbf{True Negative (TN)}: The model incorrectly identified a video as containing a kickflip.
	\item \textbf{False Positive (FP)}: The model correctly identified a video as not containing a kickflip.
	\item \textbf{False Negative (FN)}: The model incorrectly identified a video as not containing a kickflip.
\end{itemize}


\noindent \textbf{Accuracy:} Measures the proportion of correctly classified
instances, over the total number of predictions made by the model. This metric
provides a generic indicator of the model's reliability, however this specific
metric can be sensitive in scenarios with class imbalance
\cite{classificationAssessmentMethodstharwat2020}.

\begin{equation}
	\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

\noindent \textbf{Precision:} Measures the proportion of predicted positive
instances that are truly real positives
\cite{Evaluation:fromprecisionrecallandF-measuretoROCinformednessmarkednessandcorrelation}.
In the context of kickflip detection, it represents the proportion of true
kickflips against all predicted kickflips.
\begin{equation}
	\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\noindent \textbf{Recall:} Measures the proportion of positive instances that
are truly predicted positive
\cite{Evaluation:fromprecisionrecallandF-measuretoROCinformednessmarkednessandcorrelation}.In
the context of kickflip detection, it represents the proportion of true
kickflips that are identified correctly. 
\begin{equation}
	\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\noindent \textbf{F1-score:} This metric provides a balanced measure of
performance by combining both precision and recall. It is calculated using the
harmonic mean on both values and outputs a value ranged from zero to one, with
values closer to one, indicating better performance.
\begin{equation}
	\text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\noindent \textbf{Confusion Matrix:} 



