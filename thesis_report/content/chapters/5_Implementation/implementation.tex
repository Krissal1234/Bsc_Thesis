\clearpage
\chapter{Implementation}

\section{Development Environment}
{\bf Python:} Python's large database of libraries, along with its wide use in
Machine Learning, made it the ideal choice for developing this artefact.
Furthermore, its large and active community made tackling problems and
troubleshooting issues simpler. Prior experience using Python also contributed
to this decision.

{\bf Tensorflow:} Tensorflow is an open-source library, powerful in numerical
computation and Machine Learning applications. It provides a rich toolset that
allows for efficient development, training and deployment of Machine Learning
models. Notably, Tensorflow comes with the Keras API, further simplifying the
creation development by offering wide ranger of tools such as access pre-trained
models, pre-defined layers and training and evaluation abstractions.
%keras provides us with pretrained models and layers

{\bf OpenCv (cv2):} This open source library played a crucial role in the
computer vision components of development, offering essential tools related to
video processing, particularly during the frame extraction phase.

{\bf Matplotlib:} This research made use of Matplotlib for its plotting and data
visualisation functions, heavily used during the evaluation stage to properly
visualise results or any other insights gained throughout.

\section{Dataset Split and Configuration}
This study opted for splitting the original dataset into training, validation
and testing sets, comprising 80\% 10\% and 10\% respectively, in order to
prioritise training data due to limited dataset size. In consideration of Chen's
(2023) \cite{SkateboardAIPaper} research, which advocated for a sequence length
of 45 frames per video, with each frame sized at 299x299 pixels, these
parameters were initially evaluated. However, experimentation with such
parameters revealed significant computational costs and marginal improvement in
results. Consequently, this research struck a balance between costs and
performance, by opting for a sequence length of 20 and a frame size of 224x224
pixels.


%qthis study adopted a sequence length of 45, with each frame sized at 299x299 pixels, mirroring the
% methodology employed by Chen (2023) \cite{SkateboardAIPaper}. Chen's research
% demonstrated that this combination yielded superior accuracy compared to other
% configurations, involving shorter sequence length and varying frame sizes. To
% validate these findings, this research conducted experiments using a sequence
% length of 20 with 224x224 pixel sized frames. The results confirmed that a
% sequence length of 45 coupled with a larger frame size, is indeed more effective
% at achieving higher accuracies.


%  The training set was used by the models to
% learn the patterns and relationships from the input data. The validation set
% enabled is a portion of the dataset used to measure the performance after every
% epoch. Finally, the model was evaluated using the testing set.


\section{Frame Extraction using Optical Flow}
The exploration of two frame extraction techniques aimed to find the most
effective way to capture the crucial motions in a video through a series of
frames. The optical flow method was hypothesised to capture frames with the most
significant motion, unlike uniform sampling which often missed crucial parts of
the skateboard trick, capturing more frames before or after the trick execution.
An example of this behaviour at a smaller scale can be observed in Figure
\ref{fig:comparison-sequence-opticalflow-uniformSampling}. Furthermore,
experiments showed an increase in accuracy when using the optical flow method
over uniform sampling, further supporting this hypothesis.
% either prove that it led to higher accuracy or show with image comparisons
% that optical flow method has more frames encapsulating the trick

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/optical_sequence.jpg}
		\caption{Extracted frames using optical flow method.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/uniform_sampling_sequence.jpg}
		\caption{Extracted frames using uniform sampling.}
	\end{subfigure}
	\caption{Comparison of frame extraction between optical flow method and uniform sampling}
	\label{fig:comparison-sequence-opticalflow-uniformSampling}
\end{figure}

%have to mention the sequence length

This study employed Farneback's algorithm \cite{farneback2003two} using the
OpenCV (cv2) library, to estimate the optical flow magnitudes on each frame. The
selection process involved reading pairs of consecutive frames from a video,
resized for faster processing and converted to greyscale to minimise noise
caused by colour variations. The cv2 function
\texttt{cv2.calcOpticalFlowFarneback()} performed dense optical flow estimation
between these frames, using parameters such as pyramid scale, levels, winsize,
iterations that can be found in Appendix %add reference to appendix and place
values.

The function \texttt{cv2.cartToPolar()} converted the Cartesian flow vectors
returned by the optical flow function into polar coordinates, discarding the
directional information, while retaining only the magnitudes. Finally, the code
computed the average magnitude and appended it to a list. This process iterated
through all frames to select those with the highest average to be extracted from
the video.



\subsection{Data Augmentation}
This research explored a number of augmentation techniques using the
\texttt{ImageDataGenerator} library provided by Tensorflow. This process
involved defining a number of parameters that influenced the augmentation
patterns applied to each frame, as illustrated in \ref{aug-params}. In order to
expand the number of training samples, this study created multiple copies of the
original dataset, each augmented with its own set of unique parameters
controlling image shifts, brightness, scaling and other transformations.

Figure \ref{fig:originalFrames-againstAugmented} showcases an augmented trick
sequence against its original, split into six frames for illustrative purposes.
Experiments revealed that certain augmentation techniques such as image flipping
and large rotational shifts disrupted the visualisation of the trick, negatively
impacting model performance, thus these were removed or adjusted from the
augmentation parameters.

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/augmented_frames.jpg}
		\caption{Augmented frames.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/unaugmented_frames.jpg}
		\caption{Original frames.}
	\end{subfigure}
	\caption{Comparison of Augmented sequence against original}
	\label{fig:originalFrames-againstAugmented}
\end{figure}

\begin{figure}
	\begin{minted}[frame=single,
				   framesep=3mm,
				   linenos=true,
				   xleftmargin=21pt,
				   tabsize=4]{js}
	{
		"augment_params_1" = {
			'width_shift_range': 0.1,
			'height_shift_range': 0.1,
			'shear_range': 0.1,
			'rotation_range': 10,
			'zoom_range': 0.2,
			'brightness_range': (0.7, 1.3),
			'channel_shift_range': 20.0,
			'fill_mode': 'nearest'
			}
	}
	\end{minted}
	\caption{Augmentation Parameters}
	\label{aug-params}
	\end{figure}


\section{Feature Extraction and Preprocessing for Training}
%After determining the most significant frames from each video through a series
%of magnitudes and matching them with their corresponding frames, the algorithm
%moved on to the next phase of processing. Firstly, all frames were resized to
%224x224 in order to satisfy VGG and ResNet input image requirements

After successfully extracting the most significant frames from each video using
optical flow, the next step involved preparing the data for further processing.
The main algorithm consisted of an iterative process responsible for resizing and
normalising all frames before input to the pre-trained CNNs for feature
extraction. The \texttt{cv2.resize()} function resized all frames to 224x224 to
satisfy the VGG and ResNet input image requirements, before performing min-max
normalisation on each frame by diving the pixel intensities values by 255.

Once resized and normalised, the pre-trained CNNs extracted features from each
frame. With the final classification layers removed, the extracted features of a
singular image using VGG returned a shape of  ${(1 \times 7 \times 7 \times
512)}$, while ResNet50, resulted in a shape of  ${(1 \times 7 \times 7 \times
2048)}$. In both cases, the outputs were flattened using the Tensorflow
\texttt{flatten()} function to transform the extracted features into a
one-dimensional vector suitable to be fed into the dense layers of the network.
Thus, VGG generates a final feature shape of ${(20 \times 25088)}$ for an entire
video, while ResNet50 generates ${(20 \times 100352)}$.

The NumPy function \texttt{np.save()} saved these extracted features along with
their respective labels for training, validation, and test sets. This allowed
for easy retrieval of the pre-processed data, eliminating the need to re-extract
features with every new session.


\subsection{PCA Dimensionality Reduction}
This research, leveraged Principle Component Analysis (PCA) dimensionality
reduction, a technique, crucial in refining the feature sets produced by the
pre-trained models. PCA a statistical technique that transforms features into a
lower-dimensional space, where the components, known as principle components are
orthogonal to each other. These principle components are reorganised in a way
that the first few components attempt to portray maximum variance from the
original data \cite{dimensionality_reduction}.

This research, aimed to achieve 90\% to 95\% variance retention when determining
the optimal number of components for reduction. This involved plotting two
explained variance plots: one for the output of pretraining with VGG16 and
another for the output of ResNet50, as illustrated in Figure
\ref{fig:pca_plots}. These plots display the cumulative explained variance
against the number of components, allowing approximate selection of the numebr
of components by examining the 90\% to 95\% percentiles of the plots. Upon
examining both plots depicted in Figure\ref{fig:pca_plots}, 2514 components were
chosen for VGG, while 278 were selected for ResNet50. Consequently, this results
in final feature shapes of (20, 2514) and (20, 278) respectively.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/pca_vgg.png} % replace "image1" with your file name
			\caption{Caption for image 1.}
			\label{fig:pca_vgg}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/pca_resnet.png}
			\caption{Caption for image 2.}
			\label{fig:pca_resnet}
	\end{subfigure}
	\caption{Overall caption for both images.}
	\label{fig:pca_plots}
\end{figure}
%most of the important information
%explain better results from  using pca either on both vgg and resnet or just resnet after testing
%explain if we needed to use dimensionality reduction only on resnet or on both


\section{Hyperparameter Optimisation}
The architectures described in this study represent the best versions of
themselves after a number of iterations and optimisations. To speed up the time
consuming task of hyperparameter tuning, this research leveraged the power of
Optuna, a hyperparameter optimisation framework \cite{optunaFramework}. Optuna
leverages a Bayesian optimisation algorithm that runs through a number of
iterations, observing the performance of past configurations and strategically
selecting parameter combinations such as learning rate, dropout
rate and neuron count.

This iterative approach enables Optuna to navigate the search space and converge
on optimal hyperparameters for a specific model and dataset. This library was
integrated within a custom-built development environment specifically designed
to identify the optimal hyperparameter configurations for multiple model
architectures and their corresponding input data.


\section{Training History}
%show epoch against loss graphs for all 4 models
Figure shows the training and validation loss curves for the models that
achieved the best validation accuracy within each architecture. Additionally, the
vertical dashed lines indicate the epoch at which the early stopping callback
restored the models weights, preventing overfitting.

All models employed the categorical-crossentropy loss function during training.
This function measures the discrepancies between the predicted probabilities and
the actual one-hot encoded labels.


%Furthermore, the output layers utilised
% softmax activation functions to align them with the one-hot encoded labels.
% Furthermore,
%mention that they all used sgd

\subsection{Training Process and Hyperameter Tuning}
Throughout the training phase, Optuna took the responsibility of initialising
the model's hyperparameter configurations. Leveraging its bayesian optimisation
algorithm, Optuna provided initial configuraitons, which served as starting
points for the iterative training process.

In this iterative process, the model's hyperparameters underwent continuous
evaluation and refinement after every iteration at an attempt to improve the
model's performance. Performance metrics such as loss curves, confusion matrices
and F-scores, were monitored closely to assess the effectiveness of each
hyperparameter configuration. Common pitfalls observed, included overfitting,
indicated in model loss graphs when validation loss begins to rise.
Additionally, fluctuations in performance metrics such as accuracy and F-scores,
often signified an unstable learning process, suggesting the need for
reconfiguration of hyperparameters.

Additionally, with every iteration, the
hyperparameters and performance scores were saved for future analysis and
comparison, aiding in the refinement of upcoming iterations.

Common pitfalls that were looked out on


%mention SGD did better but did take longer
%Talk about how Resnet was first tested out without PCA and we came to the conclusion that the features had to be reduced

\section{Callbacks}
\subsection{Early Stopping}
An early stopping callback reduced overfitting and saved
computational resources. This method monitored the validation loss at every
epoch and halted training if improvement stopped after a predetermined patience
value. All experiments investigated a patience of 10 epochs, based on the
observation that the models were unlikely to improve after 10 epochs, with no
validation loss advancements.

\subsection{Model Checkpoint}
This study incorporated model check pointing in the training process
to save intermediate models after every epoch. This implementation was
configured to monitor the validation loss and only save the model when it
showed an improvement, allowing a seamless resumption of training in case of
interruption.

\section{Baseline Model Approach}
A baseline model approach defined during development of the artefact allowed
comparisons to be made with each iteration of the models. The first baseline
model
