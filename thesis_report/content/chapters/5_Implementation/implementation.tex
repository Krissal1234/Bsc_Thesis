\clearpage
\chapter{Implementation}
This chapter describes the practical application of the numerous strategies that
were described in the previous chapter to test the hypothesis of this study.
Together with additional sections outlining the development environment and the
training process, this chapter describes how the previously established
techniques were implemented.


% It details how the previously defined techniques were implemented and
% includes additional sections describing the training process.
% This section details on many of the previously defined methods with additional
% sections describing the training process.

\section{Development Environment}
{\bf Python:} Python's large database of libraries, along with its wide use in
Machine Learning, made it the ideal choice for developing this artefact.
Furthermore, its large and active community made tackling problems and
troubleshooting issues simpler. Prior experience using Python also contributed
to this decision.

\vspace{3mm}
\noindent
{\bf Tensorflow:} Tensorflow is an open-source library, powerful in numerical
computation and Machine Learning applications. It provides a rich toolset that
allows for efficient development, training and deployment of Machine Learning
models. Notably, Tensorflow comes with the Keras API, further simplifying the
creation development by offering wide range of tools such as access to pre-trained
models, pre-defined layers and training and evaluation abstractions.
%keras provides us with pretrained models and layers

\vspace{3mm}
\noindent
{\bf OpenCV (cv2):} This open source library played a crucial role in the
computer vision components of development, offering essential tools related to
video processing, particularly during the frame extraction phase.

\vspace{3mm}
\noindent
{\bf Matplotlib:} This research made use of the Matplotlib library for its
plotting and data visualisation functions, particularly during the evaluation
stage to visualise results or any other insights gained throughout.

\section{Dataset Split and Configuration}
This study opted for a training-validation-testing split on the original dataset
allocating 80\% for training, 10\% for validation and 10\% for testing. The
training set allowed the models to learn the patterns and relationships within
the input data. The validation enabled a portion of the data to be used to
evaluate the performance after every epoch. Finally, testing set provided a
final assessment of the model's generalizability, by using unseen data. The
decision for this split stemmed from prioritising training data due to limited
dataset size.

In consideration of Chen's (2023)~\cite{SkateboardAIPaper} research, which
advocated for a sequence length of 45 frames per video, with each frame sized at
\texttt{299\times299} pixels, these parameters were initially assessed. However,
experimentation with such parameters revealed significant computational costs
with negligible improvement in results. Consequently, this research struck a
balance between costs and performance, by opting for a sequence length of 20 and
a frame size of \texttt{224\times224} pixels.


%qthis study adopted a sequence length of 45, with each frame sized at 299x299 pixels, mirroring the
% methodology employed by Chen (2023) \cite{SkateboardAIPaper}. Chen's research
% demonstrated that this combination yielded superior accuracy compared to other
% configurations, involving shorter sequence length and varying frame sizes. To
% validate these findings, this research conducted experiments using a sequence
% length of 20 with 224x224 pixel sized frames. The results confirmed that a
% sequence length of 45 coupled with a larger frame size, is indeed more effective
% at achieving higher accuracies.


\section{Frame Extraction using Optical Flow}
The exploration of two frame extraction techniques aimed to find the most
effective way to capture the crucial motions in a video through a series of
frames. The Optical Flow method was hypothesised to capture frames with the most
significant motion, unlike uniform sampling which often missed crucial parts of
the skateboard trick, capturing more frames before or after the trick execution.
An example of this behaviour at a smaller scale can be observed in
Figure~\ref{fig:comparison-sequence-opticalflow-uniformSampling}, where the
sequence extracted using Optical Flow provides a more descriptive frame,
indicated by the green box located in the third frame. This observation is more
noticeable with larger sequence lengths.

% Furthermore,
% experiments showed an increase in accuracy when using the Optical Flow method
% over uniform sampling, further supporting this hypothesis.
% either prove that it led to higher accuracy or show with image comparisons
% that Optical Flow method has more frames encapsulating the trick

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/optical_sequence.jpg}
		\caption{Extracted frames using Optical Flow method.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/uniform_sampling_sequence.jpg}
		\caption{Extracted frames using uniform sampling.}
	\end{subfigure}
	\caption{Comparison of frame extraction between Optical Flow method and uniform sampling.}\label{fig:comparison-sequence-opticalflow-uniformSampling}
\end{figure}

%have to mention the sequence length

This study employed Farneback's algorithm~\cite{farneback2003two} using the
OpenCV (cv2) library, to estimate the Optical Flow magnitudes on each frame. The
selection process involved reading pairs of successive frames from a video,
resized for faster processing and converted to greyscale to minimise noise
caused by colour variations. The cv2 function
\texttt{cv2.calcOpticalFlowFarneback()} performed dense Optical Flow estimation
between these frames, using parameters such as pyramid scale, levels, winsize,
iterations that can be found in Appendix~\ref{tab:farneback_parameters}.

The function \texttt{cv2.cartToPolar()} converted the cartesian flow vectors
returned by the Optical Flow function into polar coordinates, discarding the
directional information, while retaining only the magnitudes. Finally, the code
computed the average magnitude and appended it to a list. This process iterated
through this list to select all frames with the highest average magnitude to be
extracted from the video.



\section{Data Augmentation}
This research explored a number of augmentation techniques using the
\texttt{ImageDataGenerator} library provided by Tensorflow. This tool allowed
for the definition of specific parameters that influenced how each frame was
augmented. To expand the number of training samples, this study created two
copies of the original dataset, each augmented with its own set of unique
parameters controlling image shifts, brightness, scaling and other
transformations. The two unique augmentation parameters utilised in this study
are illustrated in Figure~\ref{fig:augmentation_parameters}.

Figure~\ref{fig:originalFrames-againstAugmented} showcases an augmented trick
sequence against its original, split into six frames for illustrative purposes.
Experimentation revealed that certain augmentation parameters such as image
flipping and large rotational shifts disrupted the visualisation of the trick,
these were removed or adjusted to a smaller scale from the augmentation parameters.

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/augmented_frames.jpg}
		\caption{Augmented frames.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/unaugmented_frames.jpg}
		\caption{Original frames.}
	\end{subfigure}
	\caption{Comparison of Augmented sequence against original}\label{fig:originalFrames-againstAugmented}
\end{figure}

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.48\textwidth}
			\begin{minted}[frame=single,
										 framesep=2mm,
										 linenos=true,
										 fontsize=\footnotesize,
										 tabsize=4,
										 bgcolor=white]{json}
{
	"width_shift_range": 0.1,
	"height_shift_range": 0.1,
	"shear_range": 0.1,
	"rotation_range": 10,
	"zoom_range": 0.2,
	"brightness_range": [0.7, 1.3],
	"channel_shift_range": 20.0,
	"fill_mode": "nearest"
}
			\end{minted}
			\caption{Augmentation Parameters 1.}\label{fig:augment_params_1}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.48\textwidth}
			\begin{minted}[frame=single,
										 framesep=2mm,
										 linenos=true,
										 fontsize=\footnotesize,
										 tabsize=4,
										 bgcolor=white]{json}
{
	"width_shift_range": 0.2,
	"height_shift_range": 0.1,
	"shear_range": 0.8,
	"rotation_range": 15,
	"zoom_range": 0.2,
	"brightness_range": [0.2, 0.1],
	"channel_shift_range": 10.0,
	"fill_mode": "nearest"
}
			\end{minted}
			\caption{Augmentation Parameters 2}\label{fig:augment_params_2.}
	\end{subfigure}
	\caption{Augmentation Parameters for all augmentation}\label{fig:augmentation_parameters}
\end{figure}

\section{Feature Extraction and Preprocessing for Training}
%After determining the most significant frames from each video through a series
%of magnitudes and matching them with their corresponding frames, the algorithm
%moved on to the next phase of processing. Firstly, all frames were resized to
%224x224 in order to satisfy VGG and ResNet input image requirements

After successfully extracting the most significant frames from each video using
Optical Flow, the next step involved preparing the data for further processing.
The main algorithm consisted of an iterative process responsible for resizing
and normalising all frames before input to the pre-trained CNNs for feature
extraction. The \texttt{cv2.resize()} function resized all frames to
\texttt{224\times224} to satisfy the VGG16 and ResNet50 input image
requirements, before performing min-max normalisation on each frame by diving
the pixel intensities values by 255.

Once resized and normalised, the pre-trained CNNs extracted features from each
frame. With the final classification layers removed, the extracted features of a
singular image using VGG returned a shape of  ${(1 \times 7 \times 7 \times
512)}$, while ResNet50, resulted in a shape of  ${(1 \times 7 \times 7 \times
2048)}$. In both cases, the outputs were flattened using the Tensorflow
\texttt{flatten()} function to transform the extracted features into a
one-dimensional vector suitable to be fed into the dense layers of the network.
Thus, VGG generates a final feature shape of ${(20 \times 25088)}$ for an entire
video, while ResNet50 generates ${(20 \times 100352)}$.

The NumPy function \texttt{np.save()} saved these extracted features along with
their respective labels for training, validation, and test sets. This allowed
for easy retrieval of the preprocessed data, eliminating the need to re-extract
features with every new session.


\section{PCA Dimensionality Reduction}\label{5_dimensionality_reduction}
This research, leveraged Principal Component Analysis (PCA) dimensionality
reduction, a technique, used in refining the feature sets produced by the
pre-trained models.
%  PCA is a statistical technique that transforms features into
% a lower-dimensional space, by transforming the original features into a new set
% of variables known as principal components. These principal components are
% orthogonal to each other, meaning that they are uncorrelated to one another and
% are reorganised in a way that the first few components represents the maximum
% variance from the original data \cite{dimensionality_reduction}.

It was aimed to achieve around 95\% variance retention when determining the
optimal number of components for reduction. This involved plotting two explained
variance plots: one for the output of pre-training with VGG16 and another for
the output of ResNet50, as illustrated in Figure~\ref{fig:pca_plots}. These
plots display the cumulative explained variance against the number of
components, allowing approximate selection of the number of components by
examining the 95\% percentile from both plots. Upon examining both plots, 2514
components were chosen for VGG16, while 278 were selected for ResNet50.
Consequently, this results in final feature shapes of ${(20 \times 2514)}$ and
${(20 \times 278)}$ respectively.

The application of PCA dimensionality reduction significantly reduced training
times. This efficiency significantly sped up the process of hyperparameters
tuning by reducing waiting times associated with training each iteration.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/pca_vgg.png} % replace "image1" with your file name
			\caption{Cumulative variance plot of the output of VGG16.}\label{fig:pca_vgg}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/pca_resnet.png}
			\caption{Cumulative variance plot of the output of ResNet50.}\label{fig:pca_resnet}
	\end{subfigure}
	\caption{Cumulative variance plots for the outputs of VGG16 and ResNet50.}\label{fig:pca_plots}
\end{figure}


% By transforming the high-dimensional feature space extracted by
% pre-trained models into a lower-dimensional one, PCA effectively reduced the
% computational costs during training.

\section{Hyperparameter Optimisation}
The architectures described in this study represent the best versions of
themselves after a number of iterations and optimisations. To speed up the
time-consuming task of hyperparameter tuning, this research leveraged a
hyperparameter optimisation framework called Optuna~\cite{optunaFramework}. This
framework leverages a Bayesian optimisation algorithm that runs through a number
of iterations, observing the performance of past configurations and
strategically selecting parameter combinations such as learning rate, dropout
rate and neuron count. With this iterative approach, Optuna can navigate the
search space and converge on the optimal hyperparameters for a specific model
architecture.
%  This library was
% integrated within a custom-built development environment specifically designed
% to identify the optimal hyperparameter configurations for multiple model
% architectures and their corresponding input data.
\vspace{3cm}
\subsection{Training Process and Hyperparameter Tuning}
Throughout the training phase, Optuna was used to initialise the model's
hyperparameter configurations. Leveraging its Bayesian optimisation algorithm,
Optuna provided the initial configurations, which served as a starting point for
the iterative training process.

In this iterative process, each model's hyperparameters underwent continuous
evaluation and refinement after every iteration at an attempt to improve
accuracy. Performance metrics such as loss curves, confusion matrices and
F-scores, were monitored closely to assess the effectiveness of each
hyperparameter configuration.

Common pitfalls were observed, including overfitting and fluctuations in
performance metrics. Model loss graphs revealed overfitting when validation loss
increased while training loss continued to decrease. While fluctuations in
performance metrics such as accuracy and F-scores, often signified unstable
learning processes, necessitating hyperparameter reconfiguration. Finally, with
every iteration, the hyperparameters and performance metrics were saved for
future analysis and comparison, aiding in the refinement of future iterations.

% , indicated in model loss graphs
% when validation loss begins to rise, and fluctuations in performance metrics
% such as accuracy and F-scores. These fluctuations often signify an unstable
% learning process, suggesting the need for hyperparameters reconfiguration.


\section{Callbacks}
Callbacks served as essential tools, used to monitor and influence the training
process dynamically. They provided ways to automate certain tasks at different
phases of training, such as saving checkpoints of the model or stopping the
model early.

\subsection{Early Stopping}
An early stopping callback reduced overfitting and saved
computational resources. This method monitored the validation loss at every
epoch and halted training if improvement stopped after a predetermined patience
value. All experiments investigated a patience of 10 epochs, based on the
observation that the models were unlikely to improve after 10 epochs, with no
validation loss advancements.

\subsection{Model Checkpoint}
This study incorporated model check pointing in the training process
to save intermediate models after every epoch. This implementation was
configured to monitor the validation loss and only save the model when it
showed an improvement, allowing a resumption of training in case of
interruption.

\section{Training History}
Figure~\ref{fig:model_losses} shows the training and validation loss curves for
the models that achieved the best test accuracy within each architecture.
The vertical dashed lines indicate the epoch at which the early
stopping callback restored the model's weights, preventing overfitting.

The loss curves across the four architectures all exhibit distinct patterns,
highlighting the differences in learning dynamics specific to each architecture.
Notably, smaller learning rates were applied to the VGG16-LSTM and
ResNet50-BiLSTM models, with the intention to enhance the stability of training,
thereby improving performance. Additionally, the validation loss for the BiLSTM
configurations generally exhibited more volatility, likely due to the complexity
of their bidirectional layers.

All models employed the Stochastic Gradient Descent (SGD) optimiser throughout
the training process due to excessive overfitting observed with the Adam
optimiser, evident in Section 6.1.1. While SGD performed better than Adam in the
context of classifying skateboard tricks, it required longer training times to
converge.
%maybe talk about categorical crossentropy and the one hot encoded labels
% All models made use of Early Stopping to cut training short when the models
% began to overfit. This is evident in Figure \ref{fig:model_losses}, where the
% validation loss of each model began to rise significantly, indicating a decline
% in performance on unseen data.
%something about all models rised up a lot towardsthe end but early stopping did its job
\vspace{-0.5cm}
\begin{figure}[ht!]
	\centering
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/vgg_lstm_loss.png}
			\caption{VGG16-LSTM}%\label{fig:vgg_lstm_loss}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/vgg_bilstm_loss.png}
			\caption{VGG16-BiLSTM}%\label{fig:vgg_bilstm_loss}
	\end{subfigure}
	\vspace{0.5cm} % Add vertical space between rows
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/resnet_lstm_loss.png}
			\caption{ResNet50-LSTM}%\label{fig:resnet_lstm_loss}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/resnet_bilstm_loss.png}
			\caption{ResNet50-BiLSTM}%\label{fig:resnet_bilstm_loss}
	\end{subfigure}
	\caption{Loss graphs for each architecture}\label{fig:model_losses}
\end{figure}



% Table \ref{tab:pre-trained_time} provides an overview of the training times of
% applying the pre-trained models VGG16 and ResNet50 on the dataset to extract
% their features.

%talk about feature extraciton legnth
% table on training times and also feature extraction times
%also talk aboout the reason for the quick training times, due to pca and smaller learning rates

%Furthermore, the output layers utilised
% softmax activation functions to align them with the one-hot encoded labels.
% Furthermore,
%mention that they all used sgd

%mention SGD did better but did take longer
%Talk about how Resnet was first tested out without PCA and we came to the conclusion that the features had to be reduced




% \section{Baseline Model Approach}
% A baseline model approach defined during development of the artefact allowed
% comparisons to be made with each iteration of the models. The first baseline
% model
