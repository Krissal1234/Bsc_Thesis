\clearpage
\chapter{Implementation}

\section{Development Environment}
{\bf Python:} Python's large database of libraries, along with its wide use in Machine Learning, made it the ideal choice for developing this artefact. Furthermore, its large and active community made tackling problems and troubleshooting issues simpler. Prior experience using Python also contributed to this decision.

{\bf Tensorflow:} Tensorflow is an open-source library, powerful in numerical computation and Machine Learning applications. It provides a rich toolset that allows for efficient development, training and deployment of Machine learning models. Notably, Tensorflow comes with the Keras API, further simplifying the creation development by offering wide ranger of tools such as access pre-trained models
%keras provides us with pretrained models and layers 

{\bf OpenCv (cv2):} This open source library aided  

{\bf Matplotlib:} This research made use of Matplotlib for its effective plotting and data visualisation functions, heavily used during the evaluation stage to properly visualise results or any other insights gained throughout. 


\section{Frame Extraction using Optical Flow}
%have to mention how the dataset was split
The exploration of two frame extraction techniques aimed to find the most
effective way to capture the crucial motions in a video through a series of
frames. The optical flow method was hypothesised to be a superior technique for
its ability to capture frames with the most significant motion, unlike uniform
sampling which often missed crucial parts of the skateboard trick, capturing
more frames before or after the trick execution. An example of this behaviour
can be observed in Figure
\ref{fig:comparison-sequence-opticalflow-uniformSampling}. Furthermore,
experiments showed an increase in accuracy when using the optical flow method
over uniform sampling, further supporting this hypothesis.
% either prove that it led to higher accuracy or show with image comparisons that optical flow method has more frames encapsulating the trick

This study employed Farneback's algorithm \cite{farneback2003two} using the OpenCV (cv2) library, to estimate the optical flow magnitudes on each frame. The selection process involved reading pairs of consecutive frames from a video, resized for faster processing and converted to greyscale to minimise noise caused by colour variations. The cv2 function \texttt{cv2.calcOpticalFlowFarneback()} performed dense optical flow estimation between these frames, using parameters such as pyramid scale, levels, winsize, iterations that can be found in Appendix %add reference to appendix and place values. 

The function \texttt{cv2.cartToPolar()} converted the Cartesian flow vectors returned by the optical flow function into polar coordinates, discarding the directional information, retaining only the magnitudes. Finally, the code computed the average magnitude and appended it to a list. This process iterated through all frames to select those with the highest average to be extracted from the video.

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/optical_sequence.jpg}
		\caption{Extracted frames using optical flow method.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/uniform_sampling_sequence.jpg}
		\caption{Extracted frames using uniform sampling.}
	\end{subfigure}
	\caption{Comparison of frame extraction between optical flow method and uniform sampling}
	\label{fig:comparison-sequence-opticalflow-uniformSampling}
\end{figure}

\section{Dataset Split}
This study opted for splitting the original dataset into 70\% 20\% and 10\% for
training, validation and testing sets respectively. The training set was used by
the models to learn the patterns and releationships from the input data. The
validation set enabled is a portion of the dataset used to measure the performance after
every epoch. Finally, the model was evaluated using the testing set.


\subsection{Data Augmentation}
This research explored a number of augmentation techniques using the
\texttt{ImageDataGenerator} library provided by Tensorflow. This process
involved defining a number of parameters that influenced the augmentation
patterns applied to each frame. In order to expand the number of training
samples, this study created multiple copies of the original dataset, each
augmented with its own set of unique parameters controlling image shifts,
brightness, scaling and other transformations. Figure \ref{fig:originalFrames-againstAugmented} shows an example of an augmented duplication of a trick sequence against its original, split into six frames for illustrative purposes
 Conducted experiments revealed
that certain augmentation techniques such as image flipping and large rotational
shifts disrupted the visualisation of the trick, negatively impacting model
performance.

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/augmented_frames.jpg}
		\caption{Augmented frames.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/unaugmented_frames.jpg}
		\caption{Original frames.}
	\end{subfigure}
	\caption{Comparison of Augmented sequence against original}
	\label{fig:originalFrames-againstAugmented}
\end{figure}
\section{Feature Extraction and Preprocessing for Training}
%After determining the most significant frames from each video through a series
%of magnitudes and matching them with their corresponding frames, the algorithm
%moved on to the next phase of processing. Firstly, all frames were resized to
%224x224 in order to satisfy VGG and ResNet input image requirements

After successfully extracting the most significant frames from each video using
optical flow, the next step involved preparing the data for further processing.
The main algorithm consisted of an iterative process responsible for resizing and
normalising all frames before being fed to the pre-trained CNNs for feature
extraction. The \texttt{cv2.resize()} function resized all frames to 224x224 to
satisfy the VGG and ResNet input image requirements, before performing min-max
normalisation on each frame by diving the pixel intensities values by 255. 

Once resized and normalised, The pre-trained CNNs extracted features from each
frame. With the final classification layers removed, the extracted features of a
singular image using VGG returned a shape of  ${(1 \times 7 \times 7 \times
512)}$, while ResNet50, resulted in a shape of  ${(1 \times 7 \times 7 \times
2048)}$. In both cases, the outputs were flattened using the Tensorflow
\texttt{flatten()} function to transform the extracted features into a
one-dimensional vector suitable to be fed into the dense layers of the network.
Thus, VGG generates a final feature shape of ${(20 \times 25088)}$ for an entire
video, while ResNet50 generates ${(20 \times 100352)}$.

The NumPy function \texttt{np.save()} saved these extracted features along with
their respective labels for training, validation, and test sets. This allowed
for easy retrieval of the pre-processed data, eliminating the need to re-extract
features with every new session.





%adding rotations lead to worse performance due to the sequential nature of the trick
% within the  loop there was a check to see whether thecurrent video would be augmenmted
%ImageDataGenerator tensorflow module
% show parameters for augmentation plus photos of how they were augmented
%explain that data augmentation helped me train with more data, due to the lmited dataset size

\subsection{PCA Dimensionality Reduction}
This research, utilised the Principle Component Analysis (PCA) dimensionality
reduction technique which was crucial in refining the feature sets
produced by the pre-trained models. PCA a statistical technique that utilises
orthogonal transformation to convert observations of variables that may be
correlated into a set of unrelated variables called principle components. These
principle components are reorganised in a way that the first few components contain most
of the important information \cite{dimensionality_reduction}.
%explain how we got to the constant 2514 for VGG maybe? the mathematical
%explain better results from  using pca either on both vgg and resnet or just resnet after testing



\section{Hyperparameter Optimisation}
The architectures described in this study represent the best versions of
themselves after a number of iterations and optimisations. To speed up the time
consuming task of hyperparameter tuning, this research leveraged the power of
Optuna, a hyperparameter optimisation framework \cite{optunaFramework}. Optuna
leverages a Bayesian optimisation algorithm that runs through a number of
iterations, observing the performance of past configurations and strategically
selecting promising parameter combinations such as learning rate, dropout
rate and number of nodes per layer. This iterative approach enables Optuna to navigate the
search space and converge on optimal hyperparameters for a specific model and
dataset. This library was integrated within a custom-built development
environment specifically designed to identify the optimal hyperparameter
configurations for multiple model architectures and their corresponding input data.

% \section{Models}

% \subsection{LSTM}

% \subsection{BiLSTM}
%talk about the number of layers etc...
%if layers/parameters differed for different architectures
\section{Training History}
%show epoch against loss graphs for all 4 models

%This might need to go in evaluation, and only mention it in the implementation
This study compared the results of Stochastic Gradient Descent (SGD) and Adam \cite{adam_paper} as
optimisation algorithms during training. The results suggest that SGD may be
more effective at reducing overfitting compared to Adam on all architectures.
This improvement is illustrated in Figure \ref{fig:loss_graphs}, using the VGG-BiLSTM architecture.
While, Adam initially shows promise with good generalisation, it quickly
encounters a pitfall, where the validation loss begins to climb, indicating
overfitting. In contrast, the SGD-trained model exhibits a more stable
validation loss, likely due to its algorithmic stability and its capability to
achieve small generalisation errors as explained in the study by Hardt et al.
(2016) \cite{SGD_stability_paper}. The results observed with the VGG-BiLSTM
aligns with the insights discussed by Hardt et al. suggesting that
models trained using SGD are less vulnerable to overfitting.

\begin{figure}[ht!]
	\centering
	% First image (Adam)
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/Adam_optimiser.PNG} 
		\caption{Training and Validation Loss using Adam}
		\label{fig:adam_loss}
	\end{subfigure}
	\hfill
	% Second image (SGD)
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/SGD_optimiser.PNG}
		\caption{Training and Validation Loss using SGD}
		\label{fig:sgd_loss}
	\end{subfigure}
	% Overall caption
	\caption{Model loss graphs for (a) Adam and (b) SGD optimisers.}
	\label{fig:loss_graphs}
\end{figure}


\section{Callbacks}
\subsection{Early Stopping}
This research employed an early stopping callback to reduce overfitting and save
computational resources. This method monitored the validation loss at every
epoch and halted training if improvement stopped after a predetermined patience
value. This study investigated a patience of 10 epochs, based on the
observation that the models were unlikely to improve after 10 epochs, with no
validation loss advancements.

\subsection{Model Checkpoint}
This study incorporated model check pointing in the training process
to save intermediate models after every epoch. This implementation was
configured to monitor the validation loss and only save the model when it
showed an improvement, allowing a seamless resumption of training in case of
interruption.

\section{Baseline Model Approach}
This research, made use of a baseline model approach during development of the
artefact. The establishment of a baseline model allowed comparisons to be made
with each iteration of the models. The first baseline model