\clearpage
\chapter{Implementation}

\section{Development Environment}
{\bf Python:} Python's large database of libraries, along with its wide use in
Machine Learning, made it the ideal choice for developing this artefact.
Furthermore, its large and active community made tackling problems and
troubleshooting issues simpler. Prior experience using Python also contributed
to this decision.

\noindent
{\bf Tensorflow:} Tensorflow is an open-source library, powerful in numerical
computation and Machine Learning applications. It provides a rich toolset that
allows for efficient development, training and deployment of Machine Learning
models. Notably, Tensorflow comes with the Keras API, further simplifying the
creation development by offering wide ranger of tools such as access pre-trained
models, pre-defined layers and training and evaluation abstractions.
%keras provides us with pretrained models and layers

\noindent
{\bf OpenCv (cv2):} This open source library played a crucial role in the
computer vision components of development, offering essential tools related to
video processing, particularly during the frame extraction phase.

\noindent
{\bf Matplotlib:} This research made use of Matplotlib for its plotting and data
visualisation functions, heavily used during the evaluation stage to properly
visualise results or any other insights gained throughout.

\section{Dataset Split and Configuration}
This study opted for a training-validation-testing split on the original dataset
allocating 80\% for training, 10\% for validation and 10\% for testing. The
training set allowed the models to learn the patterns and relationships within
the input data. The validation enabled a portion of the data to be used to
evaluate the performance after every epoch. Finally, testing set provided a
final assessment of the model's generalizability, by using data the model's have
not seen before. The decision for this split stemmed from prioritising training
data due to limited dataset size.

In consideration of Chen's (2023) \cite{SkateboardAIPaper} research, which
advocated for a sequence length of 45 frames per video, with each frame sized at
299\times299 pixels, these parameters were initially evaluated. However,
experimentation with such parameters revealed significant computational costs
with negligible improvement in results. Consequently, this research struck a
balance between costs and performance, by opting for a sequence length of 20 and
a frame size of 224\times224 pixels.


%qthis study adopted a sequence length of 45, with each frame sized at 299x299 pixels, mirroring the
% methodology employed by Chen (2023) \cite{SkateboardAIPaper}. Chen's research
% demonstrated that this combination yielded superior accuracy compared to other
% configurations, involving shorter sequence length and varying frame sizes. To
% validate these findings, this research conducted experiments using a sequence
% length of 20 with 224x224 pixel sized frames. The results confirmed that a
% sequence length of 45 coupled with a larger frame size, is indeed more effective
% at achieving higher accuracies.


\section{Frame Extraction using Optical Flow}
The exploration of two frame extraction techniques aimed to find the most
effective way to capture the crucial motions in a video through a series of
frames. The optical flow method was hypothesised to capture frames with the most
significant motion, unlike uniform sampling which often missed crucial parts of
the skateboard trick, capturing more frames before or after the trick execution.
An example of this behaviour at a smaller scale can be observed in Figure
\ref{fig:comparison-sequence-opticalflow-uniformSampling}. Furthermore,
experiments showed an increase in accuracy when using the optical flow method
over uniform sampling, further supporting this hypothesis.
% either prove that it led to higher accuracy or show with image comparisons
% that optical flow method has more frames encapsulating the trick

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/optical_sequence.jpg}
		\caption{Extracted frames using optical flow method.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/uniform_sampling_sequence.jpg}
		\caption{Extracted frames using uniform sampling.}
	\end{subfigure}
	\caption{Comparison of frame extraction between optical flow method and uniform sampling}
	\label{fig:comparison-sequence-opticalflow-uniformSampling}
\end{figure}

%have to mention the sequence length

This study employed Farneback's algorithm \cite{farneback2003two} using the
OpenCV (cv2) library, to estimate the optical flow magnitudes on each frame. The
selection process involved reading pairs of consecutive frames from a video,
resized for faster processing and converted to greyscale to minimise noise
caused by colour variations. The cv2 function
\texttt{cv2.calcOpticalFlowFarneback()} performed dense optical flow estimation
between these frames, using parameters such as pyramid scale, levels, winsize,
iterations that can be found in Appendix %add reference to appendix and place
values.

The function \texttt{cv2.cartToPolar()} converted the Cartesian flow vectors
returned by the optical flow function into polar coordinates, discarding the
directional information, while retaining only the magnitudes. Finally, the code
computed the average magnitude and appended it to a list. This process iterated
through all frames to select those with the highest average to be extracted from
the video.



\subsection{Data Augmentation}
This research explored a number of augmentation techniques using the
\texttt{ImageDataGenerator} library provided by Tensorflow. This process
involved defining a number of parameters that influenced the augmentation
patterns applied to each frame, as illustrated in \ref{aug-params}. In order to
expand the number of training samples, this study created multiple copies of the
original dataset, each augmented with its own set of unique parameters
controlling image shifts, brightness, scaling and other transformations.

Figure \ref{fig:originalFrames-againstAugmented} showcases an augmented trick
sequence against its original, split into six frames for illustrative purposes.
Experiments revealed that certain augmentation techniques such as image flipping
and large rotational shifts disrupted the visualisation of the trick, negatively
impacting model performance, thus these were removed or adjusted from the
augmentation parameters.

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/augmented_frames.jpg}
		\caption{Augmented frames.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/unaugmented_frames.jpg}
		\caption{Original frames.}
	\end{subfigure}
	\caption{Comparison of Augmented sequence against original}
	\label{fig:originalFrames-againstAugmented}
\end{figure}

\begin{figure}
	\begin{minted}[frame=single,
				   framesep=3mm,
				   linenos=true,
				   xleftmargin=21pt,
				   tabsize=4]{js}
	{
		"augment_params_1" = {
			'width_shift_range': 0.1,
			'height_shift_range': 0.1,
			'shear_range': 0.1,
			'rotation_range': 10,
			'zoom_range': 0.2,
			'brightness_range': (0.7, 1.3),
			'channel_shift_range': 20.0,
			'fill_mode': 'nearest'
			}
	}
	\end{minted}
	\caption{Augmentation Parameters}
	\label{aug-params}
	\end{figure}


\section{Feature Extraction and Preprocessing for Training}
%After determining the most significant frames from each video through a series
%of magnitudes and matching them with their corresponding frames, the algorithm
%moved on to the next phase of processing. Firstly, all frames were resized to
%224x224 in order to satisfy VGG and ResNet input image requirements

After successfully extracting the most significant frames from each video using
optical flow, the next step involved preparing the data for further processing.
The main algorithm consisted of an iterative process responsible for resizing and
normalising all frames before input to the pre-trained CNNs for feature
extraction. The \texttt{cv2.resize()} function resized all frames to 224x224 to
satisfy the VGG and ResNet input image requirements, before performing min-max
normalisation on each frame by diving the pixel intensities values by 255.

Once resized and normalised, the pre-trained CNNs extracted features from each
frame. With the final classification layers removed, the extracted features of a
singular image using VGG returned a shape of  ${(1 \times 7 \times 7 \times
512)}$, while ResNet50, resulted in a shape of  ${(1 \times 7 \times 7 \times
2048)}$. In both cases, the outputs were flattened using the Tensorflow
\texttt{flatten()} function to transform the extracted features into a
one-dimensional vector suitable to be fed into the dense layers of the network.
Thus, VGG generates a final feature shape of ${(20 \times 25088)}$ for an entire
video, while ResNet50 generates ${(20 \times 100352)}$.

The NumPy function \texttt{np.save()} saved these extracted features along with
their respective labels for training, validation, and test sets. This allowed
for easy retrieval of the pre-processed data, eliminating the need to re-extract
features with every new session.


\subsection{PCA Dimensionality Reduction} \label{5_dimensionality_reduction}
This research, leveraged Principle Component Analysis (PCA) dimensionality
reduction, a technique, crucial in refining the feature sets produced by the
pre-trained models. PCA a statistical technique that transforms features into a
lower-dimensional space, where the components, known as principle components are
orthogonal to each other. These principle components are reorganised in a way
that the first few components attempt to portray maximum variance from the
original data \cite{dimensionality_reduction}.

This research, aimed to achieve 90\% to 95\% variance retention when determining
the optimal number of components for reduction. This involved plotting two
explained variance plots: one for the output of pre-training with VGG16 and
another for the output of ResNet50, as illustrated in Figure
\ref{fig:pca_plots}. These plots display the cumulative explained variance
against the number of components, allowing approximate selection of the number
of components by examining the 90\% to 95\% percentiles of the plots. Upon
examining both plots depicted in Figure\ref{fig:pca_plots}, 2514 components were
chosen for VGG, while 278 were selected for ResNet50. Consequently, this results
in final feature shapes of ${(20 \times 2514)}$ and ${(20 \times 278)}$ respectively.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/pca_vgg.png} % replace "image1" with your file name
			\caption{Caption for image 1.}
			\label{fig:pca_vgg}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{content/chapters/5_Implementation/figures/pca_resnet.png}
			\caption{Caption for image 2.}
			\label{fig:pca_resnet}
	\end{subfigure}
	\caption{Overall caption for both images.}
	\label{fig:pca_plots}
\end{figure}

The application of PCA dimensionality reduction significantly improved training
efficiency. By transforming the high-dimensional feature space extracted by
pre-trained models into a lower-dimensional one, PCA effectively reduced the
computational costs during training.

\section{Hyperparameter Optimisation}
The architectures described in this study represent the best versions of
themselves after a number of iterations and optimisations. To speed up the
time-consuming task of hyperparameter tuning, this research leveraged the power
of Optuna, a hyperparameter optimisation framework \cite{optunaFramework}.
Optuna leverages a Bayesian optimisation algorithm that runs through a number of
iterations, observing the performance of past configurations and strategically
selecting parameter combinations such as learning rate, dropout rate and neuron
count.

This iterative approach enables Optuna to navigate the search space and converge
on optimal hyperparameters for a specific model and dataset. This library was
integrated within a custom-built development environment specifically designed
to identify the optimal hyperparameter configurations for multiple model
architectures and their corresponding input data.

\section{Callbacks}
Callbacks served as essential tools, used to monitor and influence the training
process dynamically. They provided ways to automate certain tasks at different
phases of training, such as saving checkpoints of the model or stopping the
model early.

\subsection{Early Stopping}
An early stopping callback reduced overfitting and saved
computational resources. This method monitored the validation loss at every
epoch and halted training if improvement stopped after a predetermined patience
value. All experiments investigated a patience of 10 epochs, based on the
observation that the models were unlikely to improve after 10 epochs, with no
validation loss advancements.

\subsection{Model Checkpoint}
This study incorporated model check pointing in the training process
to save intermediate models after every epoch. This implementation was
configured to monitor the validation loss and only save the model when it
showed an improvement, allowing a seamless resumption of training in case of
interruption.

\section{Training History}
Figure \ref{fig:model_losses} displays the training and validation loss curves for
the models that achieved the best test accuracy within each architecture.
The vertical dashed lines indicate the epoch at which the early
stopping callback restored the model's weights, preventing overfitting.

The loss curves across the four architectures all exhibit distinct patterns,
highlighting the differences in learning dynamics specific to each architecture.
Notably, smaller learning rates were applied to the VGG16-LSTM and
ResNet50-BiLSTM models, with the intention to enhance the stability of training,
thereby improving performance. Additionally, the validation loss for the BiLSTM
configurations generally exhibited more volatility, likely due to the complexity
of their bidirectional layers.

%something about all models rised up a lot towardsthe end but early stopping did its job

\begin{table}[htbp]
	\centering
	\begin{tabular}{l c}
			\toprule
			\textbf{Model} & \textbf{Training Time} \\
			\midrule
			VGG 16 & 01:46:00 \\
			ResNet50 & 01:37:08 \\
			\bottomrule
	\end{tabular}
	\caption{Evaluation time (hh:mm:ss) for skateboard trick classification models.}
	\label{tab:pre-trained_time}
\end{table}

\begin{figure}[ht!]
	\centering
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/vgg_lstm_loss.png}
			\caption{VGG16-LSTM}
			\label{fig:vgg_lstm_loss}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/vgg_bilstm_loss.png}
			\caption{VGG16-BiLSTM}
			\label{fig:vgg_bilstm_loss}
	\end{subfigure}
	\vspace{0.5cm} % Add vertical space between rows
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/resnet_lstm_loss.png}
			\caption{ResNet50-LSTM}
			\label{fig:resnet_lstm_loss}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/resnet_bilstm_loss.png}
			\caption{ResNet50-BiLSTM}
			\label{fig:resnet_bilstm_loss}
	\end{subfigure}

	\caption{Loss graphs for each architecture}
	\label{fig:model_losses}
\end{figure}

All models employed the Stochastic Gradient Descent (SGD) optimiser throughout
the training process due to excessive overfitting observed with Adam in Section
6.1.1. While SGD performed better than Adam in the context of classifying
skateboard tricks, it required longer training times to converge. %maybe talk about categorical crossentropy and the one hot encoded labels


Table \ref{tab:pre-trained_time} provides an overview of the training times of
applying the pre-trained models VGG16 and ResNet50 on the dataset to extract
their features.

%talk about feature extraciton legnth
% table on training times and also feature extraction times
%also talk aboout the reason for the quick training times, due to pca and smaller learning rates

%Furthermore, the output layers utilised
% softmax activation functions to align them with the one-hot encoded labels.
% Furthermore,
%mention that they all used sgd

%mention SGD did better but did take longer
%Talk about how Resnet was first tested out without PCA and we came to the conclusion that the features had to be reduced

\subsection{Training Process and Hyperameter Tuning}
Throughout the training phase, Optuna took the responsibility of initialising
the model's hyperparameter configurations. Leveraging its Bayesian optimisation
algorithm, Optuna provided initial configurations, which served as starting
points for the iterative training process.

In this iterative process, each model's hyperparameters underwent continuous
evaluation and refinement after every iteration at an attempt to improve
accuracy. Performance metrics such as loss curves, confusion matrices and
F-scores, were monitored closely to assess the effectiveness of each
hyperparameter configuration.

Common pitfalls observed, included overfitting, indicated in model loss graphs
when validation loss begins to rise, and fluctuations in performance metrics
such as accuracy and F-scores. These fluctuations often signify an unstable
learning process, suggesting the need for hyperparameters reconfiguration.
Finally, with every iteration, the hyperparameters and performance scores were
saved for future analysis and comparison, aiding in the refinement of upcoming
iterations.



% \section{Baseline Model Approach}
% A baseline model approach defined during development of the artefact allowed
% comparisons to be made with each iteration of the models. The first baseline
% model
