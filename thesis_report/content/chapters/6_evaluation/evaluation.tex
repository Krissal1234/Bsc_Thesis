\clearpage
\chapter{Evaluation}
\section{Model Training Specifications}
Due to the absence of a dedicated graphics card, training exclusively relied on
the CPU, featuring a Ryzen 5 3600 6-core processor, operating at a base clock
speed of 3.59 GHz. Despite lacking GPU acceleration, the Ryzen 5 3600 processor
reliably handled the training of several deep learning architectures and
experiments conducted.

\section{Experiments}
\subsection{Choice of Optimiser}
This study compared the results of Stochastic Gradient Descent (SGD) and Adam
\cite{adam_paper} as optimisation algorithms during training. The results
suggest that SGD may be more effective at reducing overfitting compared to Adam.
This improvement is illustrated in Figure \ref{fig:loss_graphs}, using the
VGG-BiLSTM architecture. While, Adam initially shows promise with good
generalisation, it quickly encounters a pitfall, where the validation loss
begins to climb, indicating overfitting. In contrast, the SGD-trained model
exhibits a more stable validation loss, likely due to its algorithmic stability
and its capability to achieve small generalisation errors as explained in the
study by Hardt et al. (2016) \cite{SGD_stability_paper}. The results observed
with the VGG-BiLSTM aligns with the insights discussed by Hardt et al.
suggesting that models trained using SGD are less vulnerable to overfitting.

\begin{figure}[ht!]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/Adam_optimiser.PNG}
		\caption{Training and Validation Loss using Adam}
		\label{fig:adam_loss}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/SGD_optimiser.PNG}
		\caption{Training and Validation Loss using SGD}
		\label{fig:sgd_loss}
	\end{subfigure}
	\caption{Model loss graphs for (a) Adam and (b) SGD optimisers.}
	\label{fig:loss_graphs}
\end{figure}

\subsection{Results}

\begin{table}[htbp]
	\centering
	\begin{tabularx}{\textwidth}{>{\RaggedRight}Xcc}
			\toprule
			\textbf{Model} & \textbf{Accuracy} & \textbf{Training Time} \\
			\midrule
			\textbf{1.  VGG16-LSTM} & 86\% & 00:11:38 \\
			\textbf{2.  VGG16-BiLSTM} & 88\% & 00:18:05 \\
			\textbf{3.  ResNet50-LSTM} & 80\% & 00:02:19 \\
			\textbf{4.  ResNet50-BiLSTM} & 86\% & 00:03:01 \\
			\bottomrule
	\end{tabularx}
	\caption{Performance of models after data augmentation}
	\label{tab:models_results}
\end{table}

% talk about augmented and unaugmneted
% feature extractiona and without feature extraction
% talk about PCA improving accuracy on Resnet50

\subsection{The Effect of Data Augmentation}
%if I have time add augmentation x4
This research, also compared the effects of Augmenting data before feeding it
into the sequence models, as illustrated in Table \ref{tab:models_aug}.
Experiments revealed that each of the four models demonstrated improvements in
accuracies following the application of data augmentation on the input data.
Enhancements ranged from a 5\% increase in Model 3, to a 9\% increase in Models
1 and 4. On average, model accuracy increased by 7.75\% highlighting its
effectiveness in enhancing the model's ability to generalise on unseen data.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{>{\RaggedRight}Xcccc}
        \toprule
        \textbf{Model} & \multicolumn{2}{c}{\textbf{Unenhanced}} & \multicolumn{2}{c}{\textbf{Augmented}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & {\textbf{Accuracy}} & {\textbf{Training Time}} & {\textbf{Accuracy}} & {\textbf{Training Time}} \\
        \midrule
        \textbf{1.  VGG16-LSTM} & 77\% & 00:07:47 & 86\% & 00:11:38 \\
        \textbf{2.  VGG16-BiLSTM} & 80\% & 00:07:45 & 88\% & 00:18:05  \\
        \textbf{3.  ResNet50-LSTM} & 75\%& 00:01:19 & 80\% & 00:02:19 \\
        \textbf{4.  ResNet50-BiLSTM} & 77\% & 00:01:41& 86\% & 00:03:01 \\
        \bottomrule
    \end{tabularx}
    \caption{Performance comparison of models with and without augmentation}
    \label{tab:models_aug}
\end{table}

%mention shorter training times due to less data

\section{The Effect of Feature Extraction}
This section, compares the effects of feature extraction on the performance of
the selected sequence models. The tests utilised the best performing models from
both VGG16 and ResNet50 pre-trained models, tested with the application of PCA
as it has shown high accuracies, on the final models.

%Table

\section{Applicability In Real-Time applications}
For real-time applications like analysing skateboarding events, the speed at
which data is processed and evaluated is crucial. To evaluate the model's
applicability in such applications, this study measured the time taken to
classify a single 2-second video at $\sim$30fps for each architecture.

\begin{table}[htbp]
	\centering
	\begin{tabular}{l c}
			\toprule
			\textbf{Model} & \textbf{Evaluation Time (ss:ms)} \\
			\midrule
			Model 1 (VGG16-LSTM) &  09:10\\
			Model 2 (VGG16-BiLSTM) &  -\\
			Model 3 (ResNet50-LSTM) &  08:20\\
			Model 4 (ResNet50-BiLSTM) & 9:00\\
			\bottomrule
	\end{tabular}
	\caption{Evaluation time (ss:mm) for skateboard trick classification models.}
	\label{tab:evaluation_time}
\end{table}


The slow processing speeds outlined in Table \ref{tab:evaluation_time} is caused
by the computational expensive video analysis pipeline required. This pipeline
involves various steps: frame extraction using optical flow, feature extraction
with models like ResNet50 or VGG16, dimensionality reduction using PCA and
finally classifying the video from the sequence model. While this pipeline
negatively affected evaluation times, it was responsible for achieving high
accuracies and for significantly accelerating training time.

% While they are relatively slow, they are still applicable in real-time
% events
%if i evaluate feature extraction and no FE, then maybe I should just choose the best models from vgg and resnt
%so only evaluating 2 models

\section{Discussion}
%conslusions from experiments, what worked, what cost, training time
%show timings, list computer parts trained on
%compare models -which models do best
\subsection{Challenges}
%The reason it does not do so well with kickflips, is becuase some kickflip videos are filmed with the skater facing sideways, and it classifies them as shuvit, probaly because most shuvits are sideways
Trained models often encountered difficulty in differentiating between both the
ollie and kickflip compared to the pop shuvit. This challenge likely arises to
their shared visual characteristics. Both the ollie and the kickflip involve
common elements such as the initial pop of the skateboard and the absence of
rotation on the y-axis, which distinguishes them to the pop shuvit. The
confusion matrix shown in Figure \ref{fig:resnet-lstm-confusion} presents the
ResNet-LSTM model exhibiting slightly better classification accuracy for pop
shuvits compared to ollies and kickflips.

\begin{figure}[h]
	\centering
  \includegraphics[width=0.7 \textwidth]{content/chapters/6_evaluation/figures/resnet-lstm-cm.png}
  \caption{ResNet50-LSTM Confusion Matrix}
\label{fig:resnet-lstm-confusion}
\end{figure}

\section{Limitations}
This study encountered a major limitation, due to the insufficient amount of
data available for training the models. The scarcity of this data constrained
the ability to train more robust and accurate models, potentially not allowing
them to capture the full variability of real-world scenarios.
