\clearpage
\chapter{Evaluation}\label{6_evaluation}

Given that the previous chapter outlined the implementation details, this
chapter focuses on the evaluation of the final models. It presents the study's
final findings, including a comparative analysis with other research from the
literature.

\section{Model Training Specifications}
Due to the absence of a dedicated graphics card, training exclusively relied on
a Ryzen 5 3600 6-core processor, operating at a base clock speed of 3.59 GHz.
Despite lacking GPU acceleration, the Ryzen 5 3600 processor reliably handled
the training of several deep learning architectures and experiments conducted.

\section{Experiments}\label{6_experiments}
\subsection{Choice of Optimiser}
This study compared the results of Stochastic Gradient Descent (SGD) and
Adam~\cite{adam_paper} as optimisation algorithms during training. The results
suggest that SGD is more effective at reducing overfitting compared to Adam.
This improvement is illustrated in Figure~\ref{fig:loss_graphs}, using an
iteration of the VGG-BiLSTM architecture. While, Adam initially shows promise
with good generalisation, it quickly encounters a pitfall, where the validation
loss begins to climb, indicating overfitting. In contrast, the SGD-trained model
exhibits a more stable validation loss, likely due to its algorithmic stability
and its capability to achieve small generalisation errors as explained in the
study by Hardt et al. (2016)~\cite{SGD_stability_paper}. The results observed
with the VGG-BiLSTM aligns with the insights discussed by Hardt et al.
suggesting that models trained using SGD are less vulnerable to overfitting.

\begin{figure}[ht!]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/Adam_optimiser.PNG}
		\caption{Training and Validation Loss using Adam}\label{fig:adam_loss}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{content/chapters/5_Implementation/figures/SGD_optimiser.PNG}
		\caption{Training and Validation Loss using SGD}\label{fig:sgd_loss}
	\end{subfigure}
	\caption{Model loss graphs for (a) Adam and (b) SGD optimisers.}\label{fig:loss_graphs}
\end{figure}


% talk about augmented and unaugmneted
% feature extractiona and without feature extraction
% talk about PCA improving accuracy on Resnet50

\subsection{The Effect of Data Augmentation}\label{data_augmentation_6}
This research, also examined the effects of augmenting the input data before
feeding it into the respective sequence models. Experiments revealed that each
of the four models demonstrated improvements in accuracies following the
application of data augmentation on the input data. Enhancements ranged from a
5\% increase in Model 3, to a 9\% increase in Models 1 and 4. On average, model
accuracy increased by 7.75\% highlighting its effectiveness in enhancing the
model's ability to generalise on unseen data by adding more variability to the
training data. Table~\ref{tab:models_aug} showcases the comparison results
across all architectures.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{>{\RaggedRight}Xcccc}
        \toprule
        \textbf{Model} & \multicolumn{2}{c}{\textbf{Unenhanced}} & \multicolumn{2}{c}{\textbf{Augmented}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & {\textbf{Accuracy}} & {\textbf{Training Time}} & {\textbf{Accuracy}} & {\textbf{Training Time}} \\
        \midrule
        \textbf{1.  VGG16-LSTM} & 77\% & 00:07:47 & 86\% & 00:11:38 \\
        \textbf{2.  VGG16-BiLSTM} & 80\% & 00:07:45 & 88\% & 00:18:05  \\
        \textbf{3.  ResNet50-LSTM} & 75\%& 00:01:19 & 80\% & 00:02:19 \\
        \textbf{4.  ResNet50-BiLSTM} & 77\% & 00:01:41& 86\% & 00:03:01 \\
        \bottomrule
    \end{tabularx}
    \caption{Performance comparison of models with and without augmentation.}\label{tab:models_aug}
\end{table}

%mention shorter training times due to less data

\subsection{The Effect of PCA}
Another experiment conducted involved examining the impact of PCA on model
performance. The analysis compared VGG16-BiLSTM and ResNet50-BiLSTM as they were
the top-performing models from each category of pre-trained models. This
comparison focused on their accuracies before and after the application of PCA,
as well as differences in training times. To ensure fair results, both models were trained using the same augmented data,
with features reduced to their respective pre-calculated number of components
discussed in Section~\ref{5_dimensionality_reduction}. Notably, the sequence
models were not altered; therefore, their high performance with PCA can be
attributed to their prior hyperparameter tuning. This should be considered when
interpreting the results. Table~\ref{tab:pca_effect} presents a side-by-side comparison
of these models with their training times.

% thus, the models were originally tuned for the
% application of PCA, which may explain why they performed better with PCA as
% opposed to
% thus, the impact of PCA was examined while they
% were tuned for the application of PCA\@. This tuning may have contributed to the
% observed results.



\begin{table}[htbp]
	\centering
	\begin{tabularx}{\textwidth}{>{\RaggedRight}Xcccc}
			\toprule
			\textbf{Model} & \multicolumn{2}{c}{\textbf{Pre-PCA}} & \multicolumn{2}{c}{\textbf{Post-PCA}} \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5}
			& {\textbf{Accuracy}} & {\textbf{Training Time}} & {\textbf{Accuracy}} & {\textbf{Training Time}} \\
			\midrule
			\textbf{VGG16-BiLSTM} & 58\% & 04:29:00 & 88\% & 00:18:05  \\
			\textbf{ResNet50-BiLSTM} & 33\% & 03:29:00 & 86\% & 00:03:01 \\
			\bottomrule
	\end{tabularx}
	\caption{Comparison of VGG16-BiLSTM and ResNet50-BiLSTM model performances before and after PCA reduction.}\label{tab:pca_effect}
\end{table}

The application of PCA shows significant improvements, not only improving
accuracies but also reducing training times drastically. The observed
improvements are a result of the removal of redundant features in the dataset,
further enhancing the model's learning capabilities by allowing them to focus on
the most informative features.

% These results highlight
% the benefits of applying dimensionality reduction to combat the `curse of
% dimensionality', often encountered with high-dimensional data, noted in
% Section~\ref{4_dimensionality_reduction}.

%not going to test feature extraction because i think its out of scope, plus too many features so ill have to introduce pca which just makes it weirfd
%as ill have to stay calculating the number of components again ---ask
% \section{The Effect of Feature Extraction}
% The same sequence models, VGG-BiLSTM and ResNet50-BiLSTM, were further tested
% by comparing the effect the application of feature extraction and withou
% %Table


\subsection{Applicability In Real-Time applications}
For real-time applications like analysing skateboarding events, the speed at
which video data is processed and evaluated is crucial. To evaluate the model's
applicability in such applications, this study measured the time taken to
classify a single 2-second video at $\sim$30fps for each architecture, outlined
in Table~\ref{tab:evaluation_time}.

\begin{table}[htbp]
	\centering
	\begin{tabular}{l c}
			\toprule
			\textbf{Model} & \textbf{Evaluation Time (ss:ms)} \\
			\midrule
			Model 1 (VGG16-LSTM) &  09:10\\
			Model 2 (VGG16-BiLSTM) &  09:90\\
			Model 3 (ResNet50-LSTM) &  08:20\\
			Model 4 (ResNet50-BiLSTM) & 9:00\\
			\bottomrule
	\end{tabular}
	\caption{Evaluation time (ss:mm) for skateboard trick classification models.}\label{tab:evaluation_time}
\end{table}
The slow processing speeds outlined in Table~\ref{tab:evaluation_time} are
caused by the computational expensive video analysis pipeline required to
process a single video. While this pipeline negatively affected processing
speeds, it was responsible for achieving high accuracies and significantly
accelerating training time.
% evaluation. This pipeline involves various steps: frame extraction using
% Optical Flow, feature extraction with models like ResNet50 or VGG16,
% dimensionality reduction using PCA and finally classifying the video from the
% sequence model.

\section{Discussion}
%conslusions from experiments, what worked, what cost, training time
\subsection{Overview of Findings}

This study's extensive testing on various Deep Learning architectures and the
application of various preprocessing techniques, have revealed significant
improvement in accuracies and training efficiencies in skateboard trick
classification. Particularly, the VGG-BiLSTM model emerged as the top performer
with a final accuracy of 88\%. The full table of final accuracies is presented
in Table~\ref{tab:models_results}. The findings from experiments conducted in
Section~\ref{6_experiments} are as follows:

\begin{itemize}
    \item \textbf{Optimisers}: SGD was found to reduce overfitting more
    successfully than Adam, maintaining stable validation losses throughout training.
    \item \textbf{Data Augmentation}: Data Augmentation had a significant impact
    on the models, averaging with a 7.75\% increase in accuracies, demonstrating
    its value in increasing variability.
    \item \textbf{PCA}: The reduction of dimensionality to preserve informative
    features on the input data had a significant impact on training efficiencies
    and performance.
\end{itemize}

\noindent
Observations revealed that across all configurations, the BiLSTM variants
outperformed their LSTM counterparts. While the LSTM variants still demonstrated
good accuracies, the BiLSTM models demonstrated enhanced accuracies by an
average of 4\%. This improvement is likely due to their ability to process input
data both forwards and backward, enhancing their ability to recognise temporal
patterns that LSTMs may have missed.

Interestingly, despite the common preference for ResNet50 for feature extraction
due to its deeper and more complex structure, in this specific scenario ResNet50
models slightly underperformed compared to VGG16 models. This could suggest that
in the context of skateboard trick classification, ResNet50 does not necessarily
translate to better performance. These results may suggest that the simpler
capabilities of VGG16 are more effective for specific characteristics found in
skateboard tricks.

% \begin{table}[htbp]
% 	\centering
% 	\begin{tabularx}{\textwidth}{>{\RaggedRight}Xcc}
% 			\toprule
% 			\textbf{Model} & \textbf{Accuracy} & \textbf{Training Time} \\
% 			\midrule
% 			\textbf{1.  VGG16-LSTM} & 86\% & 00:11:38 \\
% 			\textbf{2.  VGG16-BiLSTM} & 88\% & 00:18:05 \\
% 			\textbf{3.  ResNet50-LSTM} & 80\% & 00:02:19 \\
% 			\textbf{4.  ResNet50-BiLSTM} & 86\% & 00:03:01 \\
% 			\bottomrule
% 	\end{tabularx}
% 	\caption{Performance of models after data augmentation}
% 	\label{tab:models_results}
% \end{table}
\begin{table}[htbp]
	\centering
	\begin{tabularx}{\textwidth}{>{\RaggedRight}Xcccc}
			\toprule
			\textbf{Model} & \multicolumn{2}{c}{\textbf{Study by Chen}} & \multicolumn{2}{c}{\textbf{This Study}} \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5}
			& {\textbf{Accuracy}} & {\textbf{Training Time}} & {\textbf{Accuracy}} & {\textbf{Training Time}} \\
			\midrule
			\textbf{1.  VGG16-LSTM} & 70\% & 00:40:08 & 86\% & 00:11:38 \\
			\textbf{2.  VGG16-BiLSTM} & 69\% & 00:39:11 & 88\% & 00:18:05 \\
			\textbf{3.  ResNet50-LSTM} & 80\% & 00:59:60 & 80\% & 00:02:19 \\
			\textbf{4.  ResNet50-BiLSTM} & 81\% & 00:59:80 & 86\% & 00:03:01 \\
			\bottomrule
	\end{tabularx}
	\caption{Performance comparison of models from Hanxiao Chen's
	study~\cite{SkateboardAIPaper} and this study.}\label{tab:models_results}
\end{table}

\vspace{-1cm}
\subsection{Comparative Analysis}
This section provides a comparative analysis between the outcomes of this study
and the findings reported by Orozco et al. Shapiee et al.\ and
Chen~\cite{skatePaper1,HARRecognitionInVideosUsingARobustCNNLSTMApproach,SkateboardAIPaper}.
Each study presents unique approaches with the aim of classifying complex
actions.

Orozco et al. (2020)~\cite{HARRecognitionInVideosUsingARobustCNNLSTMApproach}
achieved a high accuracy, up to 93\% with the VGG-LSTM architecture evaluated on
the KTH dataset. This accuracy stands out when compared to the best model's
accuracy of 88\% in this study. However, the KTH dataset comprises human actions
such as walking, jogging and running, which are objectively simpler than
skateboard tricks due to the lack of external entities.

The skateboard trick classifier implemented by Shapiee et al.
(2020)~\cite{skatePaper1} achieved high accuracies of 95\%, 92\% and 90\% using
three MobileNet, NASNetMobile and NASNetLarge as pre-trained models for feature
extraction before $k$-NN classification. These accuracies also stand out
compared to those obtained in this study. However, it's important to consider
certain limitations in Shapiee et al.'s methodology; specifically the lack of
variability in the skater performing the tricks and the filming environment, as
tricks were performed behind a white backdrop. These factors may have
contributed to such high accuracy rates. Nonetheless, the study by Shapiee et
al. (2020) may suggest that the pre-trained models used are more effective at
extracting features relevant to skateboarding.

Finally, when comparing this study's outcomes with those reported by Chen
(2023)~\cite{SkateboardAIPaper}, some notable differences in performance are
revealed. Despite employing the same architectures, this study not only achieved
higher accuracy but also significant improvements in training efficiency. The
comparison accuracies are detailed in Table~\ref{tab:models_results}. Chen's top
performing model, the ResNet50 + Attention + BiLSTM model, achieved an accuracy
of 84\%. In contrast, this study achieved a higher accuracy with an average
training time reduction of 41:04 minutes across all models.

Interestingly, Chen also demonstrated higher accuracies with models based on
ResNet50 as opposed to VGG, which contrasts the findings of this study where
VGG16 based models outperformed ResNet50 variants. This could be due to several
factors. Firstly, the two studies employed different hyperparameter
configurations, which can affect the model performance, potentially favouring
the deep ResNet50 Architecture. Secondly, compared to Chen, this research
included more training data per class. Thirdly, the use of Optical Flow and PCA
in this research, could have provided a more concentrated feature set for
training the models, leading to higher accuracies and more efficient training
times.

% favoured VGG16 by refining the input data to a more
% concentrated feature set that simpler models

% could have provided a more concentrated feature set.


% %go on about why they are different,
% The differences in model performance could be due to several factors. Firstly,

% to several
% factors such as different hyperparameter configurations, the use of PCA and augmentation


% of 88\% with the VGG16-BiLSTM model, in contrast to Chen's highest outlined
% accuracy of 84\% using the ResNet50 + Attention + BiLSTM model. In addition,
% compared to Chen, this study exhibits remarkable improvements in training time
% with an average reduction of 41:04 minutes, compared to their models.
% Interestingly, Chen also demonstrated higher accuracies with models based on the
% ResNet50 feature extractor as apposed to VGG, which contrasts the findings of
% this study where VGG16 based models outperformed ResNet50 variants.

% %go on about why they are different,
% The differences in model performance could be due to several factors. Firstly,
% the two studies employed different hyperparameter configurations such
% as learning rate, epochs, and dropout rates during training. Secondly, compared
% to Chen, this research included more training data per class. Thirdly, the use
% of Optical Flow and PCA in this research, could have provided a more
% concentrated feature set for training the models, leading to higher accuracies.

% chen avg time = 50 mins
% 8.6
% Chen,
% did not use Optical Flow frame extraction or Dimensionality Reduction
% techniques.

\subsection{Classification observations}
Notably, the trained models often encountered difficulty in differentiating
between the ollie and kickflip compared to the pop-shuvit. This challenge likely
arises due to their shared visual characteristics. Both the ollie and the
kickflip involve common elements such as the initial pop of the skateboard and
the absence of rotation around the y-axis, which distinguishes them to the
pop-shuvit. The confusion matrices illustrated in Figure~\ref{fig:model_cm}
demonstrate a favourable classification rate on pop-shuvit's across all models
compared to ollies and kickflips. Interestingly, in the classification of
pop-shuvit's, BiLSTM models demonstrated 100\% classification accuracy, while
LSTM models also performed well, they lagged slightly behind, correctly
classifying 11 out of 12 instances.



% \begin{figure}[h]
% 	\centering
%   \includegraphics[width=0.7 \textwidth]{content/chapters/6_evaluation/figures/resnet-lstm-cm.png}
%   \caption{ResNet50-LSTM Confusion Matrix}
% \label{fig:resnet-lstm-confusion}
% \end{figure}



% \vspace{-1cm}
\section{Limitations}

\textbf{Limited Data:} The insufficient amount of data available for training
the models constrained the ability to train more robust and accurate models.
This limitation potentially prohibited the models from capturing the full
variability of skateboard tricks filmed by video. Thus, likely not accounting
for various lighting conditions, camera angles, video quality and environmental
conditions. The result of not considering these aspects, potentially renders them
less suitable when applied to real-world scenarios.

\vspace{3mm}
\noindent
\textbf{Processing efficiency:} The current implementation of the video analysis
pipeline is very effective at improving accuracies and reducing training times,
however, it suffers from processing delays. This limitation stems from the
computationally expensive analysis steps required before trick evaluation. These
steps, including Optical Flow frame extraction, feature extraction and PCA
dimensionality reduction, delay the processing time, making it less suitable for
real-time applications.

\begin{figure}[ht!]
	\centering
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/6_evaluation/figures/vgg-lstm-cm.png}
			\caption{VGG16-LSTM}\label{fig:vgg_lstm_loss}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/6_evaluation/figures/vgg-bilstm-cm.png}
			\caption{VGG16-BiLSTM}\label{fig:vgg_bilstm_loss}
	\end{subfigure}
	\vspace{0.5cm} % Add vertical space between rows
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/6_evaluation/figures/resnet-lstm-cm.png}
			\caption{ResNet50-LSTM}\label{fig:resnet_lstm_loss}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{content/chapters/6_evaluation/figures/resnet-bilstm-cm.png}
			\caption{ResNet50-BiLSTM}\label{fig:resnet_bilstm_loss}
	\end{subfigure}

	\caption{Confusion matrices for each architecture.}\label{fig:model_cm}
\end{figure}

% This divergence in model performance could be attributed to several factors.
% Firstly, the two studies may have utilized different configurations of
% hyperparameters such as learning rates, dropout rates, and epoch numbers, which
% significantly impact model training and final performance. Secondly, the
% incorporation of attention mechanisms in Chen’s study, while adding model
% complexity and potential for better feature extraction, might not have been
% optimized in the same way as the simpler architectures in this study, leading to
% varying results.

% Additionally, the training datasets, though similar in content, might have
% differed in terms of data quality, quantity, and preprocessing methods. This
% study’s use of Optical Flow and PCA could have provided a more refined feature
% set for training the models, leading to better generalization and higher
% accuracies. In contrast, the specific preprocessing and augmentation techniques
% employed by Chen might have favored the ResNet50 architecture, which is
% generally more capable of deeper feature extraction compared to VGG16.

% These findings underscore the importance of comprehensive hyperparameter tuning
% and the need to adapt preprocessing strategies to the specific characteristics
% of the model architectures used. This study's results suggest that with careful
% optimization, VGG architectures can perform competitively and even outperform
% more complex models like ResNet50, especially when combined with effective
% preprocessing techniques.Vkkkkkk