\clearpage

\vspace{-1cm}
\chapter{Background}

\section{Skateboard Tricks}

% Performing a skateboard trick requres an intricate motion between the body and
% the board, demanding precise coordination and athleticism.

Skateboard tricks can be described as dynamic manoeuvres that involve complex
coordination of the skateboard and the skateboarder's body. The key to successfully performing these tricks is appropriate foot placement,
which is critical for controlling the skateboard's speed and direction. This
control allows skateboarders to manipulate the board in ways that replicate
specific tricks, showcasing their technical abilities and creativity. Some of
the most common and simple flat-ground tricks are listed as the following:

% Skateboard tricks
% can be categorised into several types, including:

% \noindent
% \textbf{Flat Ground } These tricks are executed on flat ground without any
% ramps, rails or other obstacles. Common examples include the ollie and shuvit.

% \noindent
% \textbf{Flips and Rotations:} These tricks involve rotating or flipping the
% board in mid-air, where the board flips along various axes. This category
% includes tricks like the kickflip or heelflip.

% \noindent
% \textbf{Grinds and Slides:} These tricks involve specialised obstacles like
% ledges or rails, allowing skaters to slide the board's trucks or deck along
% them. Examples include the 50-50 grind, where both trucks slide along the
% rail/ledge, and the boardslide, where the bottom of the skateboard deck slides
% along the rail.

% \noindent
% \textbf{Manuals:} These tricks are balance-focused, involving skaters balancing
% on just their front wheels (nose manual) or their back two wheels (manual).
%  thorugh complex motions of the body and , to initiate various
%  rotations and revolutions along various axes.

% Skateboard tricks are the heart and soul of skateboarding. These tricks
% originate from the dynamic orchestration of rotations and revolutions of a
% skateboard along various axes emphasising the significance of precise placement
% of a skateboarder's feet to initiate these rotations. These tricks serve as
% excellent examples of how the skateboarder's body and skateboard work in perfect
% harmony. Some common skateboard tricks include:
\begin{itemize}
    \item \textbf{Ollie:} One of the first tricks beginners learn. Where the
    skateboarder pops the tail of the board while simultaneously sliding their foot across the nose of the
    board, causing the board to level out in the air, used to jump over
    obstacles.
    \item \textbf{Kickflip:} A trick where the skateboarder flips the board
    under their feet while jumping, making it spin 360\textdegree~around the
    x-axis.
    \item \textbf{Pop-Shuvit:} A trick where the skateboarder scoops the board
    with their back foot causing a 180\textdegree~rotation around the y-axis.
\end{itemize}

\noindent
Skateboarders continually innovate and come up with new trick combinations,
contributing to the dynamic nature of the sport.


\section{Machine Learning}

Machine Learning (ML) can be defined as a field of study that explores
algorithms and statistical models employed by computer systems to execute tasks
without the need to be explicitly programmed. It is particularly applicable in
situations where the information we seek from a dataset is not interpretable,
and as the volume of available datasets continues to surge so does the demand
for machine learning~\cite{ML_Algorithms}.

Morris (2019)~\cite{UnderstandingLSTM} characterises ML as the advancement of
algorithms that progressively enhance their performance through practice,
suggesting that the more training the learning algorithm undergoes, the better
it becomes at executing tasks. Numerous critical factors shape a model's
performance within this phase, as exemplified by Budach et al.
(2022)~\cite{TheEffectsofDataQualityonMachineLearningPerformance}. Such factors
include dataset quality and diversity, data preprocessing, the selection of a
suitable model architecture, training time and the fine-tuning of
hyperparameters. The three main categories for ML models are defined as the
following~\cite{ML_Algorithms}:
\begin{itemize}
    \item \textbf{Supervised:} This learning approach involves training a
    model to make classifications based on input data that has been labelled
    with the correct label.
    \item \textbf{Unsupervised:} This learning approach focuses on discovering
    relationships within data when there are no predefined `correct' answers or
    labelled examples to guide the learning process. These models are left
    to autonomously explore and uncover structures in the data.
    \item \textbf{Reinforcement:} This type of learning consists of an agent
    that interacts with the environment and learns from constant feedback
    it receives in the form of rewards or penalities.
\end{itemize}

%\section{Object Detection}

%Object detection is a computer vision task that detects instances of objects in images and videos and maps them to a predefined class. For humans, the act of recognising and responding to objects is a trivial task as described by Watson et al. (2016) \cite{NeuralScience}, it is an essential feature that enables our performance and communication. Numerous researchers have shown a deep interest in this technology, focusing on various applications where object detection may play a major role, such as  surveillance systems, face detection and autonomous driving \cite{RecentAdvancesObjectDetection}.

%The output of an object detection model returns the location of the instance, as the object's centre or in the form of a bounding box. The research paper by Agarwal et al. (2018) \cite{RecentAdvancesObjectDetection} defines object
%detection as the following equation where an image is denoted as \(\mathcal{I}\), and \(O(I)\) represents the collection of object descriptions for objects within the image.
%that object detection is consistently defined within the context of a data set
%that consists of images mapped to a list of relevant object properties, such as

%their locations and scales, that are specified within each image. This
%definition makes references to the equation below,
%\begin{equation}
%O(I) = \{(Y^*_1, Z^*_1), \ldots, (Y^*_i, Z^*_i), \ldots, (Y^*_{N^*i},
%Z^*_{N^*i})\}
%\end{equation}


%In the above equation, each description encompasses two parts, \(Y^*_i \in\mathcal{Y}\) characterises the category or type of an object, and \(Z^*_{N^*i}\in \mathcal{Z}\) represents information about its location, size or shapewithin the image. \(\mathcal{Z}\) represents the different ways to describe anobject, this is typically done by specifying the object's centre \((x_c, y_c)\in \mathcal{R}^2\) or as a bounding box \((x_{min}, y_{min},x_{max},y_{max})\in \mathcal{R}^4\).
%By utilising these notations, according to
%\cite{RecentAdvancesObjectDetection}, object detection can be defined as the
%operation of combining an image with a set of detections.
%In this research
%paper, images are passed through an object detection model as one of the
%pre-processing steps before passing through the main AI model. This strategy is
%employed with the primary objective of utilising the bounding box output to
%facilitate the cropping of individual skateboarder frames, thus contributing to
%a notable reduction in computational overhead.


\vspace{-0.5cm}
\section{Activity Recognition}

 Activity recognition is the process of identifying and categorizing human
 activities from video sequences. Human activities involve a wide range of
 motions and interactions with objects, varying from simple isolated actions
 like dancing to more complex activities that engage multiple body parts and
 external objects such as football matches. The human ability to perceive these
 behaviours is a trivial task; yet, it is a challenging problem for computers
 due to the sequential nature and the resemblance of visual content in such
 activities~\cite{ActionRecognitionDeepBi-DirectionalLSTM,
 AReviewOfHumanActivityRecognitionMethods}.

% This should probably go to the implementation section
 %  Recognising complex human actions demands the examination of sequential data as
%  opposed to relying on single frames or images
%  \cite{ActionRecognitionDeepBi-DirectionalLSTM}. To illustrate this point,
%  consider the example of a skateboarder executing a challenging trick like the
%  "Kickflip", as depicted in Figure \ref{fig:SingleVsMultiFrameKickflip}. If we
%  were to feed a single frame of this trick into an AI model, as shown in Figure
%  \ref{fig:singleFrameKickflip}, it may misinterpret the manoeuvre as another
%  trick. Whereas, by providing the model with a sequence of frames, as shown in
%  Figure \ref{fig:MultiFrameKickflip}, it captures the entire essence of the
%  trick portraying the dynamic progression of actions such as foot placement,
%  board rotation and landing which collectively define the skateboard trick.

%  %Skater frame images
% \begin{figure}[h]
%   \begin{subfigure}{0.5\textwidth}
%     \centering
%     \includegraphics[width=0.57\linewidth]{content/chapters/2_background/figures/Tricks/single.png}
%     \caption{A Single Frame of a "Kickflip".}
%     \label{fig:singleFrameKickflip}
%   \end{subfigure}
%   \begin{subfigure}{0.5\textwidth}
%     \centering
%     \includegraphics[width=1\linewidth]{content/chapters/2_background/figures/Tricks/KickflipFrames.PNG}
%     \caption{Multiple Frames of a "Kickflip".}
%     \label{fig:MultiFrameKickflip}
%   \end{subfigure}
%   \caption{Comparison of a Single Frame and Multiple Sequential Frames.}
%   \label{fig:SingleVsMultiFrameKickflip}
% \end{figure}



\section{Neural Networks}
\subsection{Artificial Neural Networks}

Artificial Neural Networks (ANNs) are a class of Machine Learning models that
are influenced by the interconnected systems of neurons found in the nervous
system of living organisms. They consist of connected nodes capable of learning
from their environment and adapting to complex patterns in
data~\cite{FundamentalsOfNeuralNetworks}. Figure~\ref{fig:ANN_Diagram} presents
a schematic representation of an ANN\@. The diagram is organised into three
fundamental layers: the Input Layer, the Hidden Layer(s) and the Output
Layer~\cite{FundamentalsOfArtificialNeuralNetworksAndDeepLearning}.

\begin{itemize}
    \item \textbf{Input Layer:} This is the set of neurons that serve as the
    initial entry point for external data. Each input neuron in this
    layer corresponds to a specific feature or variable used in the Neural
    Network model.
    \item \textbf{Hidden Layer(s):} This is the set of neurons that are located
    between the Input and Output Layers where the network captures complete
    non-linear behaviours of data and feature transformations.
    \item \textbf{Output Layer:} This is the set of neurons that provide the
    final predictions produced by the Neural Network. Depending on how the ANN
    is configured, the final output can be continuous, binary, ordinal, or
    count.
\end{itemize}

\begin{figure}[ht]
	\centering
  \includegraphics[width=0.75 \textwidth]{content/chapters/2_background/figures/Machine Learning/ANN_Diagram2.png}
  \caption{Schematic Representation of an Artificial Neural Network. Reproduced
  from López et al. (2022)~\cite{FundamentalsOfArtificialNeuralNetworksAndDeepLearning}.}\label{fig:ANN_Diagram}
\end{figure}

\vspace{-1cm}
\subsection{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs), as described by Gu et al. (2018)~\cite{RecentAadvancesInConvolutionalNeuralNetworks} are a category of Deep
learning architectures with roots in the biological visual perception mechanisms
of living organisms. These networks have gained widespread attention for their
incredible performance in the area of image recognition and pattern recognition tasks.

% Unlike ANNs, CNNs have neurons arranged in three dimensions: width, height and
% depth.

CNNs are comprised of three types of layers: convolutional layers, pooling
layers and fully-connected layers. The convolutional layer applies a set of
filters that slide across the input data performing a localised dot
product between their weights and the corresponding values of the input data.
This process effectively extracts features from images, critical in
understanding complex patterns in images. The results are summed up to generate
a single value in the feature map. The pooling layer applies an aggregation
function such as max pooling or average pooling to create a downsampled
representation of the input data. Finally, the fully connected layer attempts to
transform the outputs from the previous layer into an output vector that
represents a score corresponding to a class
label~\cite{introductiontoCNNs,CNN_understandingwithmathematicalmodel}.

\begin{figure}[h]
	\centering
  \includegraphics[width=0.7 \textwidth]{content/chapters/2_background/figures/cnn_arch.png}
  \caption{CNN architecture comprising 5 layers. Reproduced from O'Shea and Nash (2015)~\cite{introductiontoCNNs}.}\label{fig:cnn_architecture}
\end{figure}

Figure~\ref{fig:cnn_architecture} showcases a simplified CNN architecture
consisting of five layers, designed to classify handwritten numbers.
Nonetheless, the complexity of CNNs can be scaled by stacking additional layers,
thereby increasing the network's depth to cater for more complex tasks.

\subsection{Recurrent Neural Networks}
Recurrent Neural Networks (RNNs) are a subset of Neural networks that are
designed for sequential data processing. RNNs are capable of modelling dynamic
relationships in sequential data by feeding signals from past time steps back
into the network. However, they are limited due to their inability to access
long-term data, limited to approximately ten sequential time
steps~\cite{UnderstandingLSTM}. This constraint arises from the difficulty in
handling vanishing or exploding gradients during the backpropagation procedure
when dealing with extended sequences, as discussed in prior
works~\cite{Long-termConvolutionalNeuralNetworksforVisualRecogntion}.

\subsubsection{Long Short-Term Memory Networks}
To address the limitation that RNNs encounter in capturing long-term
dependencies, Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) were
introduced as an extension to RNNs in 1997~\cite{learningPriceseTimingWithLSTM,originalLSTMPaper}. LSTM networks inherently
extend the RNNs memory, enabling them to capture dependencies across more than 1,000
time steps depending on the network's complexity. These models are also able to
tackle the vanishing gradient problem associated with RNNs by introducing gates that
regulate data flow. These gates maintain a constant error flow through
structures called constant error carousels (CECs) to ensure that gradients do
not vanish during backpropagation~\cite{UnderstandingLSTM}.

LSTMs consist of three gates: input, output and forget gates.
The input gate controls whether new data will be uploaded to the
network, the output gate manages whether current cell values contribute to the
output and the forget gate determines whether existing information will be
preserved or removed~\cite{theperformanceoflstmandbilstminforcastingtimeseries}.
Figure~\ref{fig:lstm_architecture} illustrates the architecture of an LSTM
network, displaying the interaction between the cell state and the various other
gates that regulate the flow of information through the network.

\begin{figure}[ht]
	\centering
  \includegraphics[width=0.7 \textwidth]{content/chapters/2_background/figures/lstm_architecture.jpg}
  \caption{LSTM architecture. Reproduced from Zhang et al. (2021)~\cite{lstm_image}.}\label{fig:lstm_architecture}
\end{figure} %https://d2l.ai/chapter_recurrent-modern/lstm.html reproduced from

\vspace{-1cm}
\subsubsection{Bidirectional LSTMs}
Bidirectional LSTMs (BiLSTMs) are variants of LSTMs designed to enhance the
ability to capture patterns in sequential data and improve learning long-term
dependencies. Unlike LSTMs that only process data in a single direction, BiLSTMs
utilise a different approach by applying two LSTM models to the input data. In
the first round, the LSTM is applied to the input data, and in the second round,
it is applied to the reversed input sequence.
% Furthermore, Tavakoli et al.
% (2019) \cite{theperformanceoflstmandbilstminforcastingtimeseries}, concluded
% that BiLSTMs reported better accuracies compared to regular LSTMs.


\begin{figure}[ht]
	\centering
  \includegraphics[width=0.6 \textwidth]{content/chapters/2_background/figures/bilstm_structure.png}
  \caption{BiLSTM Structure. Reproduced from Du et al. (2020)~\cite{du2020power}.}\label{fig:bilstm_structure}
\end{figure}

Figure~\ref{fig:bilstm_structure} demonstrates the structure of a BiLSTM
network, showcasing the flow of data between both LSTM layers. The
first LSTM processes the input data in the forward direction, from $x_1$ to
$x_6$, while the other LSTM in the backward direction from $x_6$ to $x_1$,
allowing the network to learn from past and future contexts. The forward LSTM
units are connected through weights $Wf_1$ and $Wf_2$, while the backward LSTM
units are connected through weights $Wb_1$ and $Wb_2$. At each time step, the
outputs from the forward and backward LSTMs are combined to produce the output
$y_t$.
% By learning from sequences in both directions, BiLSTMs are particularly
% effective in scenarios where past and future contexts are crucial.

\section{Optical Flow}
Optical Flow is an approximation technique used to estimate how objects move
across a series of images. It works by analysing the changes in pixel
brightness, which often occur due to the movement of objects. The application of
Optical Flow generates a two-dimensional vector field where every vector
represents the displacement of a point in an image, illustrating its direction
and distance of movement from one frame to the
next~\cite{optical_Flow_background,computation_of_opticalflow}.
Figure~\ref{fig:opticalflow} demonstrates an Optical Flow visualisation between
two frames, the lines appearing on the image indicate the direction and
magnitude of their movement between the frames.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.95\textwidth]{content/chapters/2_background/figures/optical_flow.png}

  \caption{Demonstration of Optical Flow between Two Consecutive Frames.
  Reproduced from K. Host and M. Ivašić-Kos~\cite{HARinSportsComputerVision}.}\label{fig:opticalflow}
\end{figure}
\vspace{-1cm}
\section{Dimensionality Reduction}
Datasets with many features are characterised as high-dimensional. Such datasets
often include redundant data, including closely related or identical variables.
The aim of dimensionality reduction is to remove these unnecessary components
and create a simplified lower-dimensional feature space. This helps reduce the
impact of redundant data by mapping the valuable data from the original features
to a smaller set of features~\cite{dimensionality_reduction_a_review}.

PCA is a widely used technique that transforms features into a
lower-dimensional space by transforming the original features into a new set of
variables known as principal components. These principal components are
orthogonal to each other, meaning that they are uncorrelated to one another and
are reorganised in a way that the first few components represent the maximum
variance from the original data~\cite{dimensionality_reduction}.

% condider talking about another technique

% Background Section: Understanding Optical Flow

% In the background section, introduce Optical Flow as a concept and discuss its
% general applications and importance in computer vision. This section should lay
% the theoretical groundwork for readers who may be unfamiliar with the term,
% explaining what Optical Flow is and how it is typically used across different
% fields.

% Example Background Text:

% Optical Flow is a key technique in computer vision used to estimate motion
% between two consecutive frames in a sequence of images or videos. The method
% relies on the assumption that the brightness of any object point in an image
% remains constant despite the motion of the point between frames. This principle
% allows for the calculation of motion vectors, which indicate the direction and
% speed of movement. Optical Flow is widely used in various applications such as
% video tracking, activity recognition, and autonomous driving, where it's crucial
% to understand the dynamic elements of a scene. This technique proves
% particularly valuable in scenarios where analyzing the nuances of movement
% provides more insights than static frame analysis alone.

% Methodology Section: Applying Optical Flow to Your Research

% In the methodology section, delve into the specifics of how Optical Flow was
% utilized in your study. Here, you should provide details about the
% implementation, including any specific algorithms used (like Lucas-Kanade or
% Horn-Schunck), parameters set, and how the data extracted using Optical Flow
% influenced the outcome of your research.

% Example Methodology Text:

% In this study, Optical Flow was used to enhance frame extraction from video
% sequences, focusing particularly on sequences where motion is critical, such as
% in capturing skateboarding tricks. Unlike uniform sampling, which divides a
% video into evenly spaced frames, the Optical Flow method assigns weights to
% frames based on the amount of pixel motion detected between them. This ensures
% that frames capturing significant motion are prioritized, highlighting the most
% crucial moments of action. For example, in analyzing a kickflip skateboarding
% trick, frames were selected for extraction based on their motion content,
% effectively capturing peak activity and providing a detailed visualization of
% motion dynamics over time.k